{"cells":[{"cell_type":"markdown","metadata":{"id":"cSIZe_0eJGt4"},"source":["install libraries"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-27T08:00:13.487487Z","iopub.status.busy":"2024-05-27T08:00:13.486751Z","iopub.status.idle":"2024-05-27T08:00:13.491457Z","shell.execute_reply":"2024-05-27T08:00:13.490410Z","shell.execute_reply.started":"2024-05-27T08:00:13.487454Z"},"id":"L87-IBWhJOKv","outputId":"15da57b8-89f5-493c-848b-4a8b79181e59","trusted":true},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('Oppositional_thinking')"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-29T08:53:18.449939Z","iopub.status.busy":"2024-05-29T08:53:18.449111Z","iopub.status.idle":"2024-05-29T08:53:31.963674Z","shell.execute_reply":"2024-05-29T08:53:31.962452Z","shell.execute_reply.started":"2024-05-29T08:53:18.449892Z"},"id":"6CqTVHUxJGuB","outputId":"e08111ca-75d0-41ee-e364-fb50309d4c8c","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting evaluate\n","  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n","Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.18.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.22.2)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n","Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: evaluate\n","Successfully installed evaluate-0.4.2\n"]}],"source":["!pip install evaluate"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-29T08:53:31.966511Z","iopub.status.busy":"2024-05-29T08:53:31.965765Z","iopub.status.idle":"2024-05-29T08:53:35.536394Z","shell.execute_reply":"2024-05-29T08:53:35.535530Z","shell.execute_reply.started":"2024-05-29T08:53:31.966438Z"},"id":"EmTP5PP2K0GA","outputId":"d9c16f3f-16bc-4895-cdac-30aa5012aab4","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]},{"data":{"text/plain":["2"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# test gpu:\n","import torch\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","print(device)\n","torch.cuda.device_count()"]},{"cell_type":"markdown","metadata":{"id":"YI4-aP4wJGuF"},"source":[]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T08:53:35.538262Z","iopub.status.busy":"2024-05-29T08:53:35.537752Z","iopub.status.idle":"2024-05-29T08:53:51.928387Z","shell.execute_reply":"2024-05-29T08:53:51.927419Z","shell.execute_reply.started":"2024-05-29T08:53:35.538232Z"},"id":"HfvFoGVnJGuG","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-29 08:53:40.334674: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-29 08:53:40.334786: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-29 08:53:40.465900: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import os\n","import torch\n","import evaluate\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from tqdm import tqdm\n","from torch.optim import Adam, RMSprop\n","from transformers  import  get_scheduler\n","from sklearn.model_selection import KFold\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from sklearn.model_selection import KFold\n","import wandb\n","from datetime import datetime\n","import os\n","import shutil\n","import random\n","import numpy as np\n","from itertools import product"]},{"cell_type":"markdown","metadata":{"id":"hahMpZ91JGuM"},"source":["utils"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-29T09:26:05.540724Z","iopub.status.busy":"2024-05-29T09:26:05.540095Z","iopub.status.idle":"2024-05-29T09:26:05.599898Z","shell.execute_reply":"2024-05-29T09:26:05.598973Z","shell.execute_reply.started":"2024-05-29T09:26:05.540690Z"},"id":"hYGZxU7UJGuN","outputId":"fbd7ee10-e8b4-48de-e4f7-02841bdd8ad4","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading official JSON /kaggle/input/dataset-oppositional/test/dataset_es_official_test_nolabels.json dataset\n","Loading official JSON /kaggle/input/dataset-oppositional/test/dataset_en_official_test_nolabels.json dataset\n"]}],"source":["# utils.py\n","def set_seed(seed):\n","    \"\"\"\n","    Sets the seed to make everything deterministic, for reproducibility of experiments\n","    Parameters:\n","    seed: the number to set the seed to\n","    Return: None\n","    \"\"\"\n","    # Random seed\n","    random.seed(seed)\n","    # Numpy seed\n","    np.random.seed(seed)\n","\n","    # Torch seed\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","    # os seed\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","\n","def remove_previous_model(folder):\n","    dirs = [x for x in os.listdir(folder) if os.path.isdir(folder+os.sep+x)]\n","    for x in dirs:\n","        shutil.rmtree(folder+os.sep+x, ignore_errors=False, onerror=None)\n","\n","\n","def product_dict(**kwargs):\n","    keys = kwargs.keys()\n","    vals = kwargs.values()\n","    for instance in product(*vals):\n","        yield dict(zip(keys, instance))\n","\n","# mydataset.py\n","class MyDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels=None, mode='train'):\n","        self.encodings = encodings\n","        if labels is None:\n","            self.labels = None  # No labels present\n","        else:\n","            self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n","        if self.labels is not None:\n","            item['labels'] = torch.tensor(self.labels[idx]).clone().detach()\n","        return item\n","\n","    def __len__(self):\n","        return len(self.encodings['input_ids'])  # Or another key that's always present\n","    \n","# datareader.py\n","import pandas as pd\n","import json\n","\n","BINARY_MAPPING_CRITICAL_POS = {'CONSPIRACY': 0, 'CRITICAL': 1}\n","BINARY_MAPPING_CONSPIRACY_POS = {'CRITICAL': 0, 'CONSPIRACY': 1}\n","\n","CATEGORY_MAPPING_CRITICAL_POS_INVERSE = {0: 'CONSPIRACY', 1: 'CRITICAL'}\n","CATEGORY_MAPPING_CONSPIRACY_POS_INVERSE = {0: 'CRITICAL', 1: 'CONSPIRACY'}\n","\n","# TRAIN_DATASET_ES=\"/kaggle/input/dataset-oppositional/dataset_oppositional/training/dataset_oppositional/dataset_es_train.json\"\n","# TRAIN_DATASET_EN=\"/kaggle/input/dataset-oppositional/dataset_oppositional/training/dataset_oppositional/dataset_en_train.json\"\n","TEST_DATASET_EN =\"/kaggle/input/dataset-oppositional/test/dataset_en_official_test_nolabels.json\"\n","TEST_DATASET_ES =\"/kaggle/input/dataset-oppositional/test/dataset_es_official_test_nolabels.json\"\n","\n","\n","class PAN24Reader:\n","    def __init__(self):\n","        pass\n","    def read_json_file(self, path):\n","        dataset=[]\n","        print(f'Loading official JSON {path} dataset')\n","        with open(path, 'r', encoding='utf-8') as file:\n","            dataset = json.load(file)\n","        return dataset\n","\n","    def load_dataset_classification(self, path, string_labels=False, positive_class='conspiracy'):\n","        dataset = self.read_json_file(path)\n","        # convert to a format suitable for classification\n","        texts = pd.Series([doc['text'] for doc in dataset])\n","        if string_labels:\n","            classes = pd.Series([doc['category'] for doc in dataset])\n","        else:\n","            if positive_class == 'conspiracy':\n","                binmap = BINARY_MAPPING_CONSPIRACY_POS\n","            elif positive_class == 'critical':\n","                binmap = BINARY_MAPPING_CRITICAL_POS\n","            else:\n","                raise ValueError(f'Unknown positive class: {positive_class}')\n","#             classes = [binmap[doc['category']] for doc in dataset]\n","#             classes = pd.Series(classes)\n","        ids = pd.Series([doc['id'] for doc in dataset])\n","#         data = pd.DataFrame({\"text\": texts, \"id\": ids, \"label\": classes})\n","        data = pd.DataFrame({\"text\": texts, \"id\": ids})\n","\n","        return data\n","\n","\n","myReader=PAN24Reader()\n","es_test_df = myReader.load_dataset_classification(TEST_DATASET_ES, string_labels=False, positive_class='conspiracy')\n","en_test_df = myReader.load_dataset_classification(TEST_DATASET_EN, string_labels=False, positive_class='conspiracy')\n"]},{"cell_type":"markdown","metadata":{"id":"LHeNe9N1JGuP"},"source":["fine_tunning.py"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T09:26:09.607337Z","iopub.status.busy":"2024-05-29T09:26:09.606934Z","iopub.status.idle":"2024-05-29T09:26:09.645447Z","shell.execute_reply":"2024-05-29T09:26:09.644477Z","shell.execute_reply.started":"2024-05-29T09:26:09.607302Z"},"id":"T4EmKT7PJGuQ","trusted":true},"outputs":[],"source":["def training(_wandb, _model, _train_data, _val_data, _learning_rate, _optimizer_name, _schedule, _epochs,\n","             _tokenizer, _batch_size=32, _padding=\"max_length\", _max_length=512, _truncation=True,\n","             _patience=10, _measure= \"accuracy\", _out=None):\n","    train_encodings = _tokenizer(_train_data[\"text\"].tolist(), max_length=_max_length, truncation=_truncation, padding=_padding, return_tensors=\"pt\")\n","    val_encodings = _tokenizer(_val_data[\"text\"].tolist(), max_length=_max_length, truncation=_truncation, padding=_padding, return_tensors=\"pt\")\n","\n","    train, val = MyDataset(train_encodings, _train_data[\"label\"].tolist()), MyDataset(val_encodings, _val_data[\"label\"].tolist())\n","\n","    train_dataloader, val_dataloader  = torch.utils.data.DataLoader(train, batch_size=_batch_size, shuffle=True), torch.utils.data.DataLoader(val, batch_size=_batch_size)\n","\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","    _wandb.log({\"divice\": str(device)})\n","    if use_cuda:\n","        model = _model.to(device)\n","        if torch.cuda.device_count() > 1:\n","            print(f\"Usando {torch.cuda.device_count()} GPUs\")\n","            model = torch.nn.DataParallel(model)\n","    best_measure, best_model_name, patience = None, None, 0\n","    training_stats = []\n","    \n","    # train_eval = evaluate.load(\"accuracy\")\n","    train_eval = evaluate.load(f\"Yeshwant123/{_measure}\")\n","\n","    lr_scheduler, optimizer = None, None\n","    #Here we can specify different methods to optmize the paarameters, initially we can consider Adam and RmsProp\n","\n","    _wandb.log({\"info\": \"Creating the Optimizer and Schedule \"})\n","\n","    lr_scheduler, optimizer = None, None\n","    if _optimizer_name == \"adam\":\n","        optimizer = Adam(_model.parameters(), lr=_learning_rate)\n","    elif _optimizer_name == \"rmsprop\":\n","        optimizer = RMSprop(_model.parameters(), lr=_learning_rate)\n","\n","    #Here we can define different learning rate schedules, to variate de learning rate in each training step. Initially we use\n","    # can use linear learning rate schedule\n","    num_training_steps = _epochs * len(_train_data)\n","    if _schedule==\"linear\":\n","        lr_scheduler = get_scheduler(_schedule, optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","\n","    for epoch in range(_epochs):\n","        if patience >= _patience: break\n","        total_loss_train, total_acc_train = 0, 0\n","        total_train_step = 0\n","        \n","        _model.train()\n","        \n","        for batch in train_dataloader:\n","            total_train_step += 1\n","            # print(\"Epoch \", epoch, \"Batch\", i)\n","            batch = {k: v.to(device) for k, v in batch.items()}\n","            outputs = model(**batch)\n","            loss = outputs.loss\n","            if loss.dim() > 0:\n","                loss = loss.mean()  # Ensure the loss is a scalar\n","            loss.backward()\n","            total_loss_train += loss.item()\n","            logits = outputs.logits\n","            predictions = torch.argmax(logits, dim=-1)\n","            train_eval.add_batch(predictions=predictions, references=batch[\"labels\"])\n","            optimizer.step()\n","            lr_scheduler.step()\n","            optimizer.zero_grad()\n","        total_acc_train = train_eval.compute()\n","\n","        total_eval_steps = 0\n","        total_loss_val, total_acc_val = 0, 0\n","        eval_metric = evaluate.load(f\"Yeshwant123/{_measure}\")\n","        model.eval()\n","        for batch in val_dataloader:\n","            total_eval_steps += 1\n","            batch = {k: v.to(device) for k, v in batch.items()}\n","            with torch.no_grad():\n","                outputs = model(**batch)\n","                loss = outputs.loss\n","                if loss.dim() > 0:\n","                    loss = loss.mean()  # Ensure the loss is a scalar\n","                total_loss_val += loss.item()\n","                logits = outputs.logits\n","                predictions = torch.argmax(logits, dim=-1)\n","            eval_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n","        total_acc_val = eval_metric.compute()\n","\n","        if best_measure is None or (best_measure < total_acc_val[_measure]):  # here you must set your save weights\n","            if best_measure == None: _wandb.log({\"info\": \"It's the first time (epoch) ******************\"})\n","            elif best_measure < total_acc_val[_measure]:\n","                _wandb.log({\"info\": \"In this epoch an improvement was achieved. (epoch) ******************\"})\n","\n","            best_measure = total_acc_val[_measure]\n","            try:\n","                os.makedirs(_out + os.sep + 'models', exist_ok=True)\n","            except OSError as error:\n","                _wandb.log({\"info\": \"Directory '%s' can not be created\"})\n","            # remove the directories\n","            remove_previous_model(_out + os.sep + 'models')\n","            best_model_name = _out + os.sep + 'models/bestmodel_epoch_{}'.format(epoch +1)\n","            _wandb.log({\"info\": \"The current best model is \" + best_model_name + \" \"+ str(best_measure)})\n","\n","            os.makedirs(best_model_name, exist_ok=True)\n","            if isinstance(model, torch.nn.DataParallel):\n","                model.module.save_pretrained(best_model_name)\n","            else:\n","                model.save_pretrained(best_model_name)\n","            patience = 0\n","        else:\n","            patience += 1\n","        training_stats.append(\n","            {\n","                'epoch': epoch + 1,\n","                'Training Loss': total_loss_train / total_train_step,\n","                'Valid. Loss': total_loss_val / total_eval_steps,\n","                f'Valid.{_measure}': total_acc_val[_measure],\n","                f'Training.{_measure}': total_acc_train[_measure]\n","            }\n","        )\n","        \n","        _wandb.log({\n","            'epoch': epoch + 1,\n","            'train_loss': total_loss_train / len(train_dataloader),\n","            f'train_{_measure}': total_acc_train[_measure],\n","            'val_loss': total_loss_val / len(val_dataloader),\n","            f'val_{_measure}': total_acc_val[_measure]\n","        })\n","\n","    if best_model_name != None:\n","        if isinstance(model, torch.nn.DataParallel):\n","            model = model.module.from_pretrained(best_model_name)\n","        else:\n","            model = model.from_pretrained(best_model_name)\n","        _wandb.log({\"info\": \"The final model used to predict the labels of the testing datasets is \" + best_model_name})\n","\n","    df_stats = pd.DataFrame(data=training_stats)\n","    df_stats = df_stats.set_index('epoch')\n","    df_stats.to_csv(_out + os.sep + \"training_stats.csv\")\n","\n","    _wandb.log({\"info\": df_stats})\n","    myplot = sns.lineplot(data=df_stats, palette=\"tab10\", linewidth=2.5)\n","    fig = myplot.get_figure()\n","    fig.savefig(_out + os.sep + 'loss-figue.png')\n","    plt.close()\n","    return model\n","\n","############################################################################################################################################################################3\n","#VALIDATION ON THE TEST SET\n","\n","# Define the validation function\n","def validate(_wandb, _model, _test_data, _tokenizer, _batch_size=32, _padding=\"max_length\", _max_length=512, _truncation=True, _measure=\"accuracy\", evaltype=False):\n","    test_encodings = _tokenizer(_test_data['text'].tolist(), max_length=_max_length, truncation=_truncation, padding=_padding, return_tensors=\"pt\")\n","    \n","    # Comprobar si hay etiquetas en los datos, asumir que no si no están presentes\n","    if 'label' in _test_data:\n","        labels = _test_data['label'].tolist()\n","    else:\n","        labels = None  # No labels provided\n","    \n","    test = MyDataset(test_encodings, labels, mode=\"predict\" if labels is None else \"train\")\n","    test_dataloader = torch.utils.data.DataLoader(test, batch_size=_batch_size)\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","    _model.to(device)\n","    if torch.cuda.device_count() > 1:\n","        print(f\"Usando {torch.cuda.device_count()} GPUs\")\n","        _model = torch.nn.DataParallel(_model)\n","\n","    _model.eval()\n","    all_predictions = []\n","\n","    with torch.no_grad():\n","        for batch in test_dataloader:\n","            batch = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n","            outputs = _model(**batch)\n","            logits = outputs.logits\n","            predictions = torch.argmax(logits, dim=-1)\n","            all_predictions.append(predictions.cpu())  # Move predictions to CPU\n","\n","    # Concatenar todas las predicciones\n","    all_predictions = torch.cat(all_predictions).numpy()  # Ensure tensor is on CPU before converting to numpy\n","    return all_predictions"]},{"cell_type":"markdown","metadata":{},"source":["BAGGING"]},{"cell_type":"markdown","metadata":{},"source":["## ENGLISH"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T09:27:27.703513Z","iopub.status.busy":"2024-05-29T09:27:27.702584Z","iopub.status.idle":"2024-05-29T09:28:17.420787Z","shell.execute_reply":"2024-05-29T09:28:17.419735Z","shell.execute_reply.started":"2024-05-29T09:27:27.703476Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact english_roberta-base_run_3_fold_0_model:latest, 478.72MB. 7 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n","Done. 0:0:6.1\n"]},{"name":"stdout","output_type":"stream","text":["Usando 2 GPUs\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact english_roberta-base_run_3_fold_1_model:latest, 478.72MB. 7 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n","Done. 0:0:4.5\n"]},{"name":"stdout","output_type":"stream","text":["Usando 2 GPUs\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact english_roberta-base_run_3_fold_2_model:latest, 478.72MB. 7 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n","Done. 0:0:4.0\n"]},{"name":"stdout","output_type":"stream","text":["Usando 2 GPUs\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact english_roberta-base_run_3_fold_3_model:latest, 478.72MB. 7 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n","Done. 0:0:3.7\n"]},{"name":"stdout","output_type":"stream","text":["Usando 2 GPUs\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact english_roberta-base_run_3_fold_4_model:latest, 478.72MB. 7 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n","Done. 0:0:5.9\n"]},{"name":"stdout","output_type":"stream","text":["Usando 2 GPUs\n"]}],"source":["import os\n","import numpy as np\n","import wandb\n","import pandas as pd\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from sklearn.metrics import matthews_corrcoef\n","from collections import Counter\n","\n","# Define SEED for reproducibility\n","SEED = 1234\n","\n","# Initialize WandB\n","wandb.login()\n","\n","# Initialize matrix to hold predictions for each fold (800 samples x 5 folds)\n","num_folds = 5\n","num_samples = len(en_test_df)\n","fold_predictions = np.zeros((num_samples, num_folds))\n","\n","lang = \"english\"\n","model_name = \"roberta-base\"\n","    \n","# Initialize WandB run context\n","run_name = f\"{lang}_{model_name}\"\n","api = wandb.Api()\n","\n","for fold in range(num_folds):\n","    # Load the model artifact from WandB\n","    artifact = api.artifact(f'davidandreuroqueta/lnr_oppositional_thinking_2024-05-28_17-09-22/english_roberta-base_run_3_fold_{fold}_model:latest')\n","    artifact_dir = artifact.download()\n","    model_path = artifact_dir\n","\n","    # Load the model and tokenizer\n","    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n","    tokenizer = AutoTokenizer.from_pretrained(model_path)\n","    model.eval()\n","\n","    predictions = validate(_wandb=wandb, _model=model, _test_data=en_test_df, _tokenizer=tokenizer, _batch_size=64, _padding=\"max_length\", _max_length=128, _truncation=True, _measure=\"mcc\", evaltype=False)\n","\n","    fold_predictions[:, fold] = predictions\n","\n","# Compute the majority vote for each sample\n","final_preds = [Counter(fold_predictions[i, :]).most_common(1)[0][0] for i in range(num_samples)]"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T09:49:45.295932Z","iopub.status.busy":"2024-05-29T09:49:45.295587Z","iopub.status.idle":"2024-05-29T09:49:45.381133Z","shell.execute_reply":"2024-05-29T09:49:45.380303Z","shell.execute_reply.started":"2024-05-29T09:49:45.295904Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Predictions have been saved to predictions.csv\n","JSON output has been saved to predictions.json\n"]}],"source":["# Convert numerical predictions to labels\n","def transform_label(pred):\n","    return \"CONSPIRACY\" if pred == 1 else \"CRITICAL\"\n","\n","# Create DataFrame with predictions\n","predictions_df = pd.DataFrame(fold_predictions, columns=[f'fold_{i}' for i in range(num_folds)])\n","predictions_df = predictions_df.map(transform_label)\n","predictions_df['majority_vote'] = list(map(transform_label, final_preds))\n","predictions_df['id'] = en_test_df['id']\n","\n","# Save DataFrame to CSV\n","output_csv_path = \"en_test_predictions.csv\"\n","predictions_df.to_csv(output_csv_path, index=False)\n","\n","# Create JSON with the required format\n","json_output = []\n","for index, row in predictions_df.iterrows():\n","    json_output.append({\"id\": row['id'], \"category\": row['majority_vote']})\n","\n","json_output_path = \"en_test_predictions.json\"\n","with open(json_output_path, 'w') as json_file:\n","    json.dump(json_output, json_file, indent=2)\n","\n","# Display DataFrame to user (for Jupyter Notebook environment)\n","# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Predictions DataFrame\", dataframe=predictions_df)\n","\n","print(f\"Predictions have been saved to {output_csv_path}\")\n","print(f\"JSON output has been saved to {json_output_path}\")"]},{"cell_type":"markdown","metadata":{},"source":["## SPANISH"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T10:00:12.075442Z","iopub.status.busy":"2024-05-29T10:00:12.075095Z","iopub.status.idle":"2024-05-29T10:01:00.285292Z","shell.execute_reply":"2024-05-29T10:01:00.284485Z","shell.execute_reply.started":"2024-05-29T10:00:12.075414Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact spanishrobertaadam-base-bne_run8fold0_model:v0, 478.92MB. 7 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n","Done. 0:0:4.8\n"]},{"name":"stdout","output_type":"stream","text":["Usando 2 GPUs\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact spanishrobertaadam-base-bne_run8fold1_model:v0, 478.92MB. 7 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n","Done. 0:0:4.2\n"]},{"name":"stdout","output_type":"stream","text":["Usando 2 GPUs\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact spanishrobertaadam-base-bne_run8fold2_model:v0, 478.92MB. 7 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n","Done. 0:0:3.5\n"]},{"name":"stdout","output_type":"stream","text":["Usando 2 GPUs\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact spanishrobertaadam-base-bne_run8fold3_model:v0, 478.92MB. 7 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n","Done. 0:0:6.6\n"]},{"name":"stdout","output_type":"stream","text":["Usando 2 GPUs\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact spanishrobertaadam-base-bne_run8fold4_model:v0, 478.92MB. 7 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n","Done. 0:0:3.2\n"]},{"name":"stdout","output_type":"stream","text":["Usando 2 GPUs\n"]}],"source":["# Define SEED for reproducibility\n","SEED = 1234\n","\n","# Initialize WandB\n","wandb.login()\n","\n","# Initialize matrix to hold predictions for each fold (800 samples x 5 folds)\n","num_folds = 5\n","num_samples = len(es_test_df)\n","fold_predictions = np.zeros((num_samples, num_folds))\n","\n","    \n","all_predictions = []\n","api = wandb.Api()\n","\n","for fold in range(num_folds):\n","    # Load the model artifact from WandB\n","    artifact = api.artifact(f'ghernandez-hector/lnr_oppositional_thinking_2024-05-28_12-47-06/spanishrobertaadam-base-bne_run8fold{fold}_model:v0')\n","    artifact_dir = artifact.download()\n","    model_path = artifact_dir\n","\n","    # Load the model and tokenizer\n","    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n","    tokenizer = AutoTokenizer.from_pretrained(model_path)\n","    model.eval()\n","\n","    predictions = validate(_wandb=wandb, _model=model, _test_data=es_test_df, _tokenizer=tokenizer, _batch_size=64, _padding=\"max_length\", _max_length=128, _truncation=True, _measure=\"mcc\", evaltype=False)\n","\n","    all_predictions.append(predictions)\n","    \n","final_preds = [Counter(fold_predictions[i, :]).most_common(1)[0][0] for i in range(num_samples)]"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T10:01:59.467674Z","iopub.status.busy":"2024-05-29T10:01:59.466812Z","iopub.status.idle":"2024-05-29T10:01:59.553977Z","shell.execute_reply":"2024-05-29T10:01:59.553165Z","shell.execute_reply.started":"2024-05-29T10:01:59.467624Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Predictions have been saved to es_test_predictions.csv\n","JSON output has been saved to es_test_predictions.json\n"]}],"source":["# Convert numerical predictions to labels\n","def transform_label(pred):\n","    return \"CONSPIRACY\" if pred == 1 else \"CRITICAL\"\n","\n","# Create DataFrame with predictions\n","predictions_df = pd.DataFrame(fold_predictions, columns=[f'fold_{i}' for i in range(num_folds)])\n","predictions_df = predictions_df.map(transform_label)\n","predictions_df['majority_vote'] = list(map(transform_label, final_preds))\n","predictions_df['id'] = es_test_df['id']\n","\n","# Save DataFrame to CSV\n","output_csv_path = \"es_test_predictions.csv\"\n","predictions_df.to_csv(output_csv_path, index=False)\n","\n","# Create JSON with the required format\n","json_output = []\n","for index, row in predictions_df.iterrows():\n","    json_output.append({\"id\": row['id'], \"category\": row['majority_vote']})\n","\n","json_output_path = \"es_test_predictions.json\"\n","with open(json_output_path, 'w') as json_file:\n","    json.dump(json_output, json_file, indent=2)\n","\n","# Display DataFrame to user (for Jupyter Notebook environment)\n","# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Predictions DataFrame\", dataframe=predictions_df)\n","\n","print(f\"Predictions have been saved to {output_csv_path}\")\n","print(f\"JSON output has been saved to {json_output_path}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5094523,"sourceId":8530315,"sourceType":"datasetVersion"},{"datasetId":5044153,"sourceId":8547743,"sourceType":"datasetVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"13e22342799a4323bf3f1059e082827f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15bad4d6a1164c70acbc8db7edbd9e66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_62607de3071e4999951dd21f2d7422c4","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_929d8dcca0024332b37e7e5c4b6ef00e","value":1}},"21f30a65139d4aa6b755660371049a03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30e03ce93eb34293bcd87ca7959854a3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3407d57d8488404eaff1306a93b3b4ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_b822b37bb5cd40e58f423e83a82a61e2","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93271ff93bec44f583c1a4622529f1ce","value":1}},"3946ec0b82444435bbef194b589ce92d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_7551706e14434d37885ece328004c5ec","IPY_MODEL_e8cff45d1e774b3bab4dd6befb10d43c"],"layout":"IPY_MODEL_e67937bd65224880a199beffec88be89"}},"3f0b92538fc04fd08cd2e1572de89f84":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42d5bd90622e4626b8f73ab130cf846f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_a027348f0aaf4a3bb4e354e9b73bd8ef","IPY_MODEL_ffdd2c63a036471ebf0692a844fc12d8"],"layout":"IPY_MODEL_f3c6d847ab604a57aa04ab3f2ba258e2"}},"62607de3071e4999951dd21f2d7422c4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7092d9f13ee9493f95362b26e23f601e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7551706e14434d37885ece328004c5ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c987fd2de8b405a8d294975214cbc3d","placeholder":"​","style":"IPY_MODEL_7092d9f13ee9493f95362b26e23f601e","value":"0.013 MB of 0.013 MB uploaded\r"}},"7921f8a3b21645d9afded2d1ef2b59ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a5d4ee7078b4f67965c4ccb6b793625":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c987fd2de8b405a8d294975214cbc3d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cedcf6632e643b383e8d9c196af693a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"929d8dcca0024332b37e7e5c4b6ef00e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"93271ff93bec44f583c1a4622529f1ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9ce3a42132254f7ea112342a71a658c6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a027348f0aaf4a3bb4e354e9b73bd8ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30e03ce93eb34293bcd87ca7959854a3","placeholder":"​","style":"IPY_MODEL_7921f8a3b21645d9afded2d1ef2b59ba","value":"0.014 MB of 0.014 MB uploaded\r"}},"aa8399df41354082b2923f1e99075ed9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f0b92538fc04fd08cd2e1572de89f84","placeholder":"​","style":"IPY_MODEL_b71b0e1ce144434c9fc9b4d9f781997c","value":"0.014 MB of 0.014 MB uploaded\r"}},"b48d92ae81684a96be46844484e156fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_e7a687efc09840a280c42bf8f3d5e4a5","IPY_MODEL_15bad4d6a1164c70acbc8db7edbd9e66"],"layout":"IPY_MODEL_21f30a65139d4aa6b755660371049a03"}},"b71b0e1ce144434c9fc9b4d9f781997c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b822b37bb5cd40e58f423e83a82a61e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccd6079762884df5b2ec6a5ffb5a47bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e18ed07e2eea4fce8dd7a06d5edc5317":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e67937bd65224880a199beffec88be89":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7a687efc09840a280c42bf8f3d5e4a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7cedcf6632e643b383e8d9c196af693a","placeholder":"​","style":"IPY_MODEL_ccd6079762884df5b2ec6a5ffb5a47bc","value":"0.014 MB of 0.014 MB uploaded\r"}},"e8cff45d1e774b3bab4dd6befb10d43c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_13e22342799a4323bf3f1059e082827f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e18ed07e2eea4fce8dd7a06d5edc5317","value":1}},"eb5dc2642e2345e0a5582a4f94451389":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ef4721235ee240d08badd81528eaf79c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_aa8399df41354082b2923f1e99075ed9","IPY_MODEL_3407d57d8488404eaff1306a93b3b4ac"],"layout":"IPY_MODEL_7a5d4ee7078b4f67965c4ccb6b793625"}},"f3c6d847ab604a57aa04ab3f2ba258e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffdd2c63a036471ebf0692a844fc12d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ce3a42132254f7ea112342a71a658c6","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eb5dc2642e2345e0a5582a4f94451389","value":1}}}}},"nbformat":4,"nbformat_minor":4}
