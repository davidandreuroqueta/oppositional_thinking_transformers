{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8530315,"sourceType":"datasetVersion","datasetId":5094523},{"sourceId":8547743,"sourceType":"datasetVersion","datasetId":5044153}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"13e22342799a4323bf3f1059e082827f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15bad4d6a1164c70acbc8db7edbd9e66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_62607de3071e4999951dd21f2d7422c4","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_929d8dcca0024332b37e7e5c4b6ef00e","value":1}},"21f30a65139d4aa6b755660371049a03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30e03ce93eb34293bcd87ca7959854a3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3407d57d8488404eaff1306a93b3b4ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_b822b37bb5cd40e58f423e83a82a61e2","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93271ff93bec44f583c1a4622529f1ce","value":1}},"3946ec0b82444435bbef194b589ce92d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_7551706e14434d37885ece328004c5ec","IPY_MODEL_e8cff45d1e774b3bab4dd6befb10d43c"],"layout":"IPY_MODEL_e67937bd65224880a199beffec88be89"}},"3f0b92538fc04fd08cd2e1572de89f84":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42d5bd90622e4626b8f73ab130cf846f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_a027348f0aaf4a3bb4e354e9b73bd8ef","IPY_MODEL_ffdd2c63a036471ebf0692a844fc12d8"],"layout":"IPY_MODEL_f3c6d847ab604a57aa04ab3f2ba258e2"}},"62607de3071e4999951dd21f2d7422c4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7092d9f13ee9493f95362b26e23f601e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7551706e14434d37885ece328004c5ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c987fd2de8b405a8d294975214cbc3d","placeholder":"â€‹","style":"IPY_MODEL_7092d9f13ee9493f95362b26e23f601e","value":"0.013 MB of 0.013 MB uploaded\r"}},"7921f8a3b21645d9afded2d1ef2b59ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a5d4ee7078b4f67965c4ccb6b793625":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c987fd2de8b405a8d294975214cbc3d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cedcf6632e643b383e8d9c196af693a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"929d8dcca0024332b37e7e5c4b6ef00e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"93271ff93bec44f583c1a4622529f1ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9ce3a42132254f7ea112342a71a658c6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a027348f0aaf4a3bb4e354e9b73bd8ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30e03ce93eb34293bcd87ca7959854a3","placeholder":"â€‹","style":"IPY_MODEL_7921f8a3b21645d9afded2d1ef2b59ba","value":"0.014 MB of 0.014 MB uploaded\r"}},"aa8399df41354082b2923f1e99075ed9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f0b92538fc04fd08cd2e1572de89f84","placeholder":"â€‹","style":"IPY_MODEL_b71b0e1ce144434c9fc9b4d9f781997c","value":"0.014 MB of 0.014 MB uploaded\r"}},"b48d92ae81684a96be46844484e156fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_e7a687efc09840a280c42bf8f3d5e4a5","IPY_MODEL_15bad4d6a1164c70acbc8db7edbd9e66"],"layout":"IPY_MODEL_21f30a65139d4aa6b755660371049a03"}},"b71b0e1ce144434c9fc9b4d9f781997c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b822b37bb5cd40e58f423e83a82a61e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccd6079762884df5b2ec6a5ffb5a47bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e18ed07e2eea4fce8dd7a06d5edc5317":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e67937bd65224880a199beffec88be89":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7a687efc09840a280c42bf8f3d5e4a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7cedcf6632e643b383e8d9c196af693a","placeholder":"â€‹","style":"IPY_MODEL_ccd6079762884df5b2ec6a5ffb5a47bc","value":"0.014 MB of 0.014 MB uploaded\r"}},"e8cff45d1e774b3bab4dd6befb10d43c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_13e22342799a4323bf3f1059e082827f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e18ed07e2eea4fce8dd7a06d5edc5317","value":1}},"eb5dc2642e2345e0a5582a4f94451389":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ef4721235ee240d08badd81528eaf79c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_aa8399df41354082b2923f1e99075ed9","IPY_MODEL_3407d57d8488404eaff1306a93b3b4ac"],"layout":"IPY_MODEL_7a5d4ee7078b4f67965c4ccb6b793625"}},"f3c6d847ab604a57aa04ab3f2ba258e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffdd2c63a036471ebf0692a844fc12d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ce3a42132254f7ea112342a71a658c6","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eb5dc2642e2345e0a5582a4f94451389","value":1}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"install libraries","metadata":{"id":"cSIZe_0eJGt4"}},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('Oppositional_thinking')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L87-IBWhJOKv","outputId":"15da57b8-89f5-493c-848b-4a8b79181e59","execution":{"iopub.status.busy":"2024-05-29T10:18:08.674671Z","iopub.execute_input":"2024-05-29T10:18:08.675078Z","iopub.status.idle":"2024-05-29T10:18:08.680045Z","shell.execute_reply.started":"2024-05-29T10:18:08.675050Z","shell.execute_reply":"2024-05-29T10:18:08.678958Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6CqTVHUxJGuB","outputId":"e08111ca-75d0-41ee-e364-fb50309d4c8c","execution":{"iopub.status.busy":"2024-05-29T10:18:08.682273Z","iopub.execute_input":"2024-05-29T10:18:08.682760Z","iopub.status.idle":"2024-05-29T10:18:21.911368Z","shell.execute_reply.started":"2024-05-29T10:18:08.682715Z","shell.execute_reply":"2024-05-29T10:18:21.910325Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.2)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.18.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# test gpu:\nimport torch\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nprint(device)\ntorch.cuda.device_count()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EmTP5PP2K0GA","outputId":"d9c16f3f-16bc-4895-cdac-30aa5012aab4","execution":{"iopub.status.busy":"2024-05-29T10:18:21.912899Z","iopub.execute_input":"2024-05-29T10:18:21.913206Z","iopub.status.idle":"2024-05-29T10:18:21.922415Z","shell.execute_reply.started":"2024-05-29T10:18:21.913176Z","shell.execute_reply":"2024-05-29T10:18:21.921339Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{"id":"YI4-aP4wJGuF"}},{"cell_type":"code","source":"import os\nimport torch\nimport evaluate\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tqdm import tqdm\nfrom torch.optim import Adam, RMSprop\nfrom transformers  import  get_scheduler\nfrom sklearn.model_selection import KFold\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.model_selection import KFold\nimport wandb\nfrom datetime import datetime\nimport os\nimport shutil\nimport random\nimport numpy as np\nfrom itertools import product","metadata":{"id":"HfvFoGVnJGuG","execution":{"iopub.status.busy":"2024-05-29T10:18:21.924649Z","iopub.execute_input":"2024-05-29T10:18:21.924951Z","iopub.status.idle":"2024-05-29T10:18:21.935360Z","shell.execute_reply.started":"2024-05-29T10:18:21.924925Z","shell.execute_reply":"2024-05-29T10:18:21.934354Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"utils","metadata":{"id":"hahMpZ91JGuM"}},{"cell_type":"code","source":"# utils.py\ndef set_seed(seed):\n    \"\"\"\n    Sets the seed to make everything deterministic, for reproducibility of experiments\n    Parameters:\n    seed: the number to set the seed to\n    Return: None\n    \"\"\"\n    # Random seed\n    random.seed(seed)\n    # Numpy seed\n    np.random.seed(seed)\n\n    # Torch seed\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\n    # os seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\ndef remove_previous_model(folder):\n    dirs = [x for x in os.listdir(folder) if os.path.isdir(folder+os.sep+x)]\n    for x in dirs:\n        shutil.rmtree(folder+os.sep+x, ignore_errors=False, onerror=None)\n\n\ndef product_dict(**kwargs):\n    keys = kwargs.keys()\n    vals = kwargs.values()\n    for instance in product(*vals):\n        yield dict(zip(keys, instance))\n\n# mydataset.py\nclass MyDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels=None, mode='train'):\n        self.encodings = encodings\n        if labels is None:\n            self.labels = None  # No labels present\n        else:\n            self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n        if self.labels is not None:\n            item['labels'] = torch.tensor(self.labels[idx]).clone().detach()\n        return item\n\n    def __len__(self):\n        return len(self.encodings['input_ids'])  # Or another key that's always present\n    \n# datareader.py\nimport pandas as pd\nimport json\n\nBINARY_MAPPING_CRITICAL_POS = {'CONSPIRACY': 0, 'CRITICAL': 1}\nBINARY_MAPPING_CONSPIRACY_POS = {'CRITICAL': 0, 'CONSPIRACY': 1}\n\nCATEGORY_MAPPING_CRITICAL_POS_INVERSE = {0: 'CONSPIRACY', 1: 'CRITICAL'}\nCATEGORY_MAPPING_CONSPIRACY_POS_INVERSE = {0: 'CRITICAL', 1: 'CONSPIRACY'}\n\n# TRAIN_DATASET_ES=\"/kaggle/input/dataset-oppositional/dataset_oppositional/training/dataset_oppositional/dataset_es_train.json\"\n# TRAIN_DATASET_EN=\"/kaggle/input/dataset-oppositional/dataset_oppositional/training/dataset_oppositional/dataset_en_train.json\"\nTEST_DATASET_EN =\"/kaggle/input/dataset-oppositional/test/dataset_en_official_test_nolabels.json\"\nTEST_DATASET_ES =\"/kaggle/input/dataset-oppositional/test/dataset_es_official_test_nolabels.json\"\n\n\nclass PAN24Reader:\n    def __init__(self):\n        pass\n    def read_json_file(self, path):\n        dataset=[]\n        print(f'Loading official JSON {path} dataset')\n        with open(path, 'r', encoding='utf-8') as file:\n            dataset = json.load(file)\n        return dataset\n\n    def load_dataset_classification(self, path, string_labels=False, positive_class='conspiracy'):\n        dataset = self.read_json_file(path)\n        # convert to a format suitable for classification\n        texts = pd.Series([doc['text'] for doc in dataset])\n        if string_labels:\n            classes = pd.Series([doc['category'] for doc in dataset])\n        else:\n            if positive_class == 'conspiracy':\n                binmap = BINARY_MAPPING_CONSPIRACY_POS\n            elif positive_class == 'critical':\n                binmap = BINARY_MAPPING_CRITICAL_POS\n            else:\n                raise ValueError(f'Unknown positive class: {positive_class}')\n#             classes = [binmap[doc['category']] for doc in dataset]\n#             classes = pd.Series(classes)\n        ids = pd.Series([doc['id'] for doc in dataset])\n#         data = pd.DataFrame({\"text\": texts, \"id\": ids, \"label\": classes})\n        data = pd.DataFrame({\"text\": texts, \"id\": ids})\n\n        return data\n\n\nmyReader=PAN24Reader()\nes_test_df = myReader.load_dataset_classification(TEST_DATASET_ES, string_labels=False, positive_class='conspiracy')\nen_test_df = myReader.load_dataset_classification(TEST_DATASET_EN, string_labels=False, positive_class='conspiracy')\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hYGZxU7UJGuN","outputId":"fbd7ee10-e8b4-48de-e4f7-02841bdd8ad4","execution":{"iopub.status.busy":"2024-05-29T10:18:21.936870Z","iopub.execute_input":"2024-05-29T10:18:21.937145Z","iopub.status.idle":"2024-05-29T10:18:22.024131Z","shell.execute_reply.started":"2024-05-29T10:18:21.937122Z","shell.execute_reply":"2024-05-29T10:18:22.023171Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Loading official JSON /kaggle/input/dataset-oppositional/test/dataset_es_official_test_nolabels.json dataset\nLoading official JSON /kaggle/input/dataset-oppositional/test/dataset_en_official_test_nolabels.json dataset\n","output_type":"stream"}]},{"cell_type":"markdown","source":"fine_tunning.py","metadata":{"id":"LHeNe9N1JGuP"}},{"cell_type":"code","source":"def training(_wandb, _model, _train_data, _val_data, _learning_rate, _optimizer_name, _schedule, _epochs,\n             _tokenizer, _batch_size=32, _padding=\"max_length\", _max_length=512, _truncation=True,\n             _patience=10, _measure= \"accuracy\", _out=None):\n    train_encodings = _tokenizer(_train_data[\"text\"].tolist(), max_length=_max_length, truncation=_truncation, padding=_padding, return_tensors=\"pt\")\n    val_encodings = _tokenizer(_val_data[\"text\"].tolist(), max_length=_max_length, truncation=_truncation, padding=_padding, return_tensors=\"pt\")\n\n    train, val = MyDataset(train_encodings, _train_data[\"label\"].tolist()), MyDataset(val_encodings, _val_data[\"label\"].tolist())\n\n    train_dataloader, val_dataloader  = torch.utils.data.DataLoader(train, batch_size=_batch_size, shuffle=True), torch.utils.data.DataLoader(val, batch_size=_batch_size)\n\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n    _wandb.log({\"divice\": str(device)})\n    if use_cuda:\n        model = _model.to(device)\n        if torch.cuda.device_count() > 1:\n            print(f\"Usando {torch.cuda.device_count()} GPUs\")\n            model = torch.nn.DataParallel(model)\n    best_measure, best_model_name, patience = None, None, 0\n    training_stats = []\n    \n    # train_eval = evaluate.load(\"accuracy\")\n    train_eval = evaluate.load(f\"Yeshwant123/{_measure}\")\n\n    lr_scheduler, optimizer = None, None\n    #Here we can specify different methods to optmize the paarameters, initially we can consider Adam and RmsProp\n\n    _wandb.log({\"info\": \"Creating the Optimizer and Schedule \"})\n\n    lr_scheduler, optimizer = None, None\n    if _optimizer_name == \"adam\":\n        optimizer = Adam(_model.parameters(), lr=_learning_rate)\n    elif _optimizer_name == \"rmsprop\":\n        optimizer = RMSprop(_model.parameters(), lr=_learning_rate)\n\n    #Here we can define different learning rate schedules, to variate de learning rate in each training step. Initially we use\n    # can use linear learning rate schedule\n    num_training_steps = _epochs * len(_train_data)\n    if _schedule==\"linear\":\n        lr_scheduler = get_scheduler(_schedule, optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\n\n    for epoch in range(_epochs):\n        if patience >= _patience: break\n        total_loss_train, total_acc_train = 0, 0\n        total_train_step = 0\n        \n        _model.train()\n        \n        for batch in train_dataloader:\n            total_train_step += 1\n            # print(\"Epoch \", epoch, \"Batch\", i)\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            if loss.dim() > 0:\n                loss = loss.mean()  # Ensure the loss is a scalar\n            loss.backward()\n            total_loss_train += loss.item()\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=-1)\n            train_eval.add_batch(predictions=predictions, references=batch[\"labels\"])\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n        total_acc_train = train_eval.compute()\n\n        total_eval_steps = 0\n        total_loss_val, total_acc_val = 0, 0\n        eval_metric = evaluate.load(f\"Yeshwant123/{_measure}\")\n        model.eval()\n        for batch in val_dataloader:\n            total_eval_steps += 1\n            batch = {k: v.to(device) for k, v in batch.items()}\n            with torch.no_grad():\n                outputs = model(**batch)\n                loss = outputs.loss\n                if loss.dim() > 0:\n                    loss = loss.mean()  # Ensure the loss is a scalar\n                total_loss_val += loss.item()\n                logits = outputs.logits\n                predictions = torch.argmax(logits, dim=-1)\n            eval_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n        total_acc_val = eval_metric.compute()\n\n        if best_measure is None or (best_measure < total_acc_val[_measure]):  # here you must set your save weights\n            if best_measure == None: _wandb.log({\"info\": \"It's the first time (epoch) ******************\"})\n            elif best_measure < total_acc_val[_measure]:\n                _wandb.log({\"info\": \"In this epoch an improvement was achieved. (epoch) ******************\"})\n\n            best_measure = total_acc_val[_measure]\n            try:\n                os.makedirs(_out + os.sep + 'models', exist_ok=True)\n            except OSError as error:\n                _wandb.log({\"info\": \"Directory '%s' can not be created\"})\n            # remove the directories\n            remove_previous_model(_out + os.sep + 'models')\n            best_model_name = _out + os.sep + 'models/bestmodel_epoch_{}'.format(epoch +1)\n            _wandb.log({\"info\": \"The current best model is \" + best_model_name + \" \"+ str(best_measure)})\n\n            os.makedirs(best_model_name, exist_ok=True)\n            if isinstance(model, torch.nn.DataParallel):\n                model.module.save_pretrained(best_model_name)\n            else:\n                model.save_pretrained(best_model_name)\n            patience = 0\n        else:\n            patience += 1\n        training_stats.append(\n            {\n                'epoch': epoch + 1,\n                'Training Loss': total_loss_train / total_train_step,\n                'Valid. Loss': total_loss_val / total_eval_steps,\n                f'Valid.{_measure}': total_acc_val[_measure],\n                f'Training.{_measure}': total_acc_train[_measure]\n            }\n        )\n        \n        _wandb.log({\n            'epoch': epoch + 1,\n            'train_loss': total_loss_train / len(train_dataloader),\n            f'train_{_measure}': total_acc_train[_measure],\n            'val_loss': total_loss_val / len(val_dataloader),\n            f'val_{_measure}': total_acc_val[_measure]\n        })\n\n    if best_model_name != None:\n        if isinstance(model, torch.nn.DataParallel):\n            model = model.module.from_pretrained(best_model_name)\n        else:\n            model = model.from_pretrained(best_model_name)\n        _wandb.log({\"info\": \"The final model used to predict the labels of the testing datasets is \" + best_model_name})\n\n    df_stats = pd.DataFrame(data=training_stats)\n    df_stats = df_stats.set_index('epoch')\n    df_stats.to_csv(_out + os.sep + \"training_stats.csv\")\n\n    _wandb.log({\"info\": df_stats})\n    myplot = sns.lineplot(data=df_stats, palette=\"tab10\", linewidth=2.5)\n    fig = myplot.get_figure()\n    fig.savefig(_out + os.sep + 'loss-figue.png')\n    plt.close()\n    return model\n\n############################################################################################################################################################################3\n#VALIDATION ON THE TEST SET\n\n# Define the validation function\ndef validate(_wandb, _model, _test_data, _tokenizer, _batch_size=32, _padding=\"max_length\", _max_length=512, _truncation=True, _measure=\"accuracy\", evaltype=False):\n    test_encodings = _tokenizer(_test_data['text'].tolist(), max_length=_max_length, truncation=_truncation, padding=_padding, return_tensors=\"pt\")\n    \n    # Comprobar si hay etiquetas en los datos, asumir que no si no estÃ¡n presentes\n    if 'label' in _test_data:\n        labels = _test_data['label'].tolist()\n    else:\n        labels = None  # No labels provided\n    \n    test = MyDataset(test_encodings, labels, mode=\"predict\" if labels is None else \"train\")\n    test_dataloader = torch.utils.data.DataLoader(test, batch_size=_batch_size)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n    _model.to(device)\n    if torch.cuda.device_count() > 1:\n        print(f\"Usando {torch.cuda.device_count()} GPUs\")\n        _model = torch.nn.DataParallel(_model)\n\n    _model.eval()\n    all_predictions = []\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            batch = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n            outputs = _model(**batch)\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=-1)\n            all_predictions.append(predictions.cpu())  # Move predictions to CPU\n\n    # Concatenar todas las predicciones\n    all_predictions = torch.cat(all_predictions).numpy()  # Ensure tensor is on CPU before converting to numpy\n    return all_predictions","metadata":{"id":"T4EmKT7PJGuQ","execution":{"iopub.status.busy":"2024-05-29T10:18:22.025851Z","iopub.execute_input":"2024-05-29T10:18:22.026236Z","iopub.status.idle":"2024-05-29T10:18:22.070534Z","shell.execute_reply.started":"2024-05-29T10:18:22.026203Z","shell.execute_reply":"2024-05-29T10:18:22.069098Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"BAGGING","metadata":{}},{"cell_type":"markdown","source":"## ENGLISH","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport wandb\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.metrics import matthews_corrcoef\nfrom collections import Counter\n\n# Define SEED for reproducibility\nSEED = 1234\n\n# Initialize WandB\nwandb.login()\n\n# Initialize matrix to hold predictions for each fold (800 samples x 5 folds)\nnum_folds = 5\nnum_samples = len(en_test_df)\nfold_predictions = np.zeros((num_samples, num_folds))\n\nlang = \"english\"\nmodel_name = \"roberta-base\"\n    \n# Initialize WandB run context\nrun_name = f\"{lang}_{model_name}\"\napi = wandb.Api()\n\nfor fold in range(num_folds):\n    # Load the model artifact from WandB\n    artifact = api.artifact(f'davidandreuroqueta/lnr_oppositional_thinking_2024-05-28_17-09-22/english_roberta-base_run_3_fold_{fold}_model:latest')\n    artifact_dir = artifact.download()\n    model_path = artifact_dir\n\n    # Load the model and tokenizer\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model.eval()\n\n    predictions = validate(_wandb=wandb, _model=model, _test_data=en_test_df, _tokenizer=tokenizer, _batch_size=64, _padding=\"max_length\", _max_length=128, _truncation=True, _measure=\"mcc\", evaltype=False)\n\n    fold_predictions[:, fold] = predictions\n\n# Compute the majority vote for each sample\nfinal_preds = [Counter(fold_predictions[i, :]).most_common(1)[0][0] for i in range(num_samples)]","metadata":{"execution":{"iopub.status.busy":"2024-05-29T10:18:22.071888Z","iopub.execute_input":"2024-05-29T10:18:22.072266Z","iopub.status.idle":"2024-05-29T10:18:56.458614Z","shell.execute_reply.started":"2024-05-29T10:18:22.072215Z","shell.execute_reply":"2024-05-29T10:18:56.457573Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdavidandreuroqueta\u001b[0m (\u001b[33mdavidandreu-org\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact english_roberta-base_run_3_fold_0_model:latest, 478.72MB. 7 files... \n\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \nDone. 0:0:1.0\n","output_type":"stream"},{"name":"stdout","text":"Usando 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact english_roberta-base_run_3_fold_1_model:latest, 478.72MB. 7 files... \n\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \nDone. 0:0:1.0\n","output_type":"stream"},{"name":"stdout","text":"Usando 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact english_roberta-base_run_3_fold_2_model:latest, 478.72MB. 7 files... \n\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \nDone. 0:0:1.0\n","output_type":"stream"},{"name":"stdout","text":"Usando 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact english_roberta-base_run_3_fold_3_model:latest, 478.72MB. 7 files... \n\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \nDone. 0:0:2.9\n","output_type":"stream"},{"name":"stdout","text":"Usando 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact english_roberta-base_run_3_fold_4_model:latest, 478.72MB. 7 files... \n\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \nDone. 0:0:4.4\n","output_type":"stream"},{"name":"stdout","text":"Usando 2 GPUs\n","output_type":"stream"}]},{"cell_type":"code","source":"# Convert numerical predictions to labels\ndef transform_label(pred):\n    return \"CONSPIRACY\" if pred == 1 else \"CRITICAL\"\n\n# Create DataFrame with predictions\npredictions_df = pd.DataFrame(fold_predictions, columns=[f'fold_{i}' for i in range(num_folds)])\npredictions_df = predictions_df.map(transform_label)\npredictions_df['majority_vote'] = list(map(transform_label, final_preds))\npredictions_df['id'] = en_test_df['id']\n\n# Save DataFrame to CSV\noutput_csv_path = \"en_test_predictions.csv\"\npredictions_df.to_csv(output_csv_path, index=False)\n\n# Create JSON with the required format\njson_output = []\nfor index, row in predictions_df.iterrows():\n    json_output.append({\"id\": row['id'], \"category\": row['majority_vote']})\n\njson_output_path = \"en_test_predictions.json\"\nwith open(json_output_path, 'w') as json_file:\n    json.dump(json_output, json_file, indent=2)\n\n# Display DataFrame to user (for Jupyter Notebook environment)\n# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Predictions DataFrame\", dataframe=predictions_df)\n\nprint(f\"Predictions have been saved to {output_csv_path}\")\nprint(f\"JSON output has been saved to {json_output_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-29T10:18:56.460607Z","iopub.execute_input":"2024-05-29T10:18:56.461010Z","iopub.status.idle":"2024-05-29T10:18:56.567367Z","shell.execute_reply.started":"2024-05-29T10:18:56.460967Z","shell.execute_reply":"2024-05-29T10:18:56.566398Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Predictions have been saved to en_test_predictions.csv\nJSON output has been saved to en_test_predictions.json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## SPANISH","metadata":{}},{"cell_type":"code","source":"# Define SEED for reproducibility\nSEED = 1234\n\n# Initialize WandB\nwandb.login()\n\n# Initialize matrix to hold predictions for each fold (800 samples x 5 folds)\nnum_folds = 5\nnum_samples = len(es_test_df)\nfold_predictions = np.zeros((num_samples, num_folds))\n\nall_predictions = []\napi = wandb.Api()\n\nfor fold in range(num_folds):\n    # Load the model artifact from WandB\n    artifact = api.artifact(f'ghernandez-hector/lnr_oppositional_thinking_2024-05-28_12-47-06/spanishrobertaadam-base-bne_run8fold{fold}_model:v0')\n    artifact_dir = artifact.download()\n    model_path = artifact_dir\n\n    # Load the model and tokenizer\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model.eval()\n\n    predictions = validate(_wandb=wandb, _model=model, _test_data=es_test_df, _tokenizer=tokenizer, _batch_size=64, _padding=\"max_length\", _max_length=128, _truncation=True, _measure=\"mcc\", evaltype=False)\n\n    fold_predictions[:, fold] = predictions\n\n# Compute the majority vote for each sample\nfinal_preds = [Counter(fold_predictions[i, :]).most_common(1)[0][0] for i in range(num_samples)]","metadata":{"execution":{"iopub.status.busy":"2024-05-29T10:18:56.570536Z","iopub.execute_input":"2024-05-29T10:18:56.570944Z","iopub.status.idle":"2024-05-29T10:19:37.233424Z","shell.execute_reply.started":"2024-05-29T10:18:56.570915Z","shell.execute_reply":"2024-05-29T10:19:37.232622Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact spanishrobertaadam-base-bne_run8fold0_model:v0, 478.92MB. 7 files... \n\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \nDone. 0:0:2.7\n","output_type":"stream"},{"name":"stdout","text":"Usando 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact spanishrobertaadam-base-bne_run8fold1_model:v0, 478.92MB. 7 files... \n\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \nDone. 0:0:3.2\n","output_type":"stream"},{"name":"stdout","text":"Usando 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact spanishrobertaadam-base-bne_run8fold2_model:v0, 478.92MB. 7 files... \n\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \nDone. 0:0:4.3\n","output_type":"stream"},{"name":"stdout","text":"Usando 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact spanishrobertaadam-base-bne_run8fold3_model:v0, 478.92MB. 7 files... \n\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \nDone. 0:0:2.9\n","output_type":"stream"},{"name":"stdout","text":"Usando 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact spanishrobertaadam-base-bne_run8fold4_model:v0, 478.92MB. 7 files... \n\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \nDone. 0:0:3.0\n","output_type":"stream"},{"name":"stdout","text":"Usando 2 GPUs\n","output_type":"stream"}]},{"cell_type":"code","source":"# Convert numerical predictions to labels\ndef transform_label(pred):\n    return \"CONSPIRACY\" if pred == 1 else \"CRITICAL\"\n\n# Create DataFrame with predictions\npredictions_df = pd.DataFrame(fold_predictions, columns=[f'fold_{i}' for i in range(num_folds)])\npredictions_df = predictions_df.map(transform_label)\npredictions_df['majority_vote'] = list(map(transform_label, final_preds))\npredictions_df['id'] = es_test_df['id']\n\n# Save DataFrame to CSV\noutput_csv_path = \"es_test_predictions.csv\"\npredictions_df.to_csv(output_csv_path, index=False)\n\n# Create JSON with the required format\njson_output = []\nfor index, row in predictions_df.iterrows():\n    json_output.append({\"id\": row['id'], \"category\": row['majority_vote']})\n\njson_output_path = \"es_test_predictions.json\"\nwith open(json_output_path, 'w') as json_file:\n    json.dump(json_output, json_file, indent=2)\n\n# Display DataFrame to user (for Jupyter Notebook environment)\n# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Predictions DataFrame\", dataframe=predictions_df)\n\nprint(f\"Predictions have been saved to {output_csv_path}\")\nprint(f\"JSON output has been saved to {json_output_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-29T10:19:37.234595Z","iopub.execute_input":"2024-05-29T10:19:37.234892Z","iopub.status.idle":"2024-05-29T10:19:37.328044Z","shell.execute_reply.started":"2024-05-29T10:19:37.234867Z","shell.execute_reply":"2024-05-29T10:19:37.327125Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Predictions have been saved to es_test_predictions.csv\nJSON output has been saved to es_test_predictions.json\n","output_type":"stream"}]}]}