{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mydataset import  MyDataset\n",
    "from utils.utils import remove_previous_model\n",
    "import os\n",
    "import torch\n",
    "import evaluate\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam, RMSprop\n",
    "from transformers  import  get_scheduler\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import KFold\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Sets the seed to make everything deterministic, for reproducibility of experiments\n",
    "    Parameters:\n",
    "    seed: the number to set the seed to\n",
    "    Return: None\n",
    "    \"\"\"\n",
    "    # Random seed\n",
    "    random.seed(seed)\n",
    "    # Numpy seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Torch seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # os seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def remove_previous_model(folder):\n",
    "    dirs = [x for x in os.listdir(folder) if os.path.isdir(folder+os.sep+x)]\n",
    "    for x in dirs:\n",
    "        shutil.rmtree(folder+os.sep+x, ignore_errors=False, onerror=None)\n",
    "        \n",
    "        \n",
    "def product_dict(**kwargs):\n",
    "    keys = kwargs.keys()\n",
    "    vals = kwargs.values()\n",
    "    for instance in product(*vals):\n",
    "        yield dict(zip(keys, instance))\n",
    "        \n",
    "# mydataset.py\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels, mode='train'):\n",
    "        self.encodings = encodings\n",
    "        if mode !=\"train\":\n",
    "            self.labels=  [0]*len(encodings)\n",
    "        else: self.labels = labels\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "# datareader.py\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "BINARY_MAPPING_CRITICAL_POS = {'CONSPIRACY': 0, 'CRITICAL': 1}\n",
    "BINARY_MAPPING_CONSPIRACY_POS = {'CRITICAL': 0, 'CONSPIRACY': 1}\n",
    "\n",
    "CATEGORY_MAPPING_CRITICAL_POS_INVERSE = {0: 'CONSPIRACY', 1: 'CRITICAL'}\n",
    "CATEGORY_MAPPING_CONSPIRACY_POS_INVERSE = {0: 'CRITICAL', 1: 'CONSPIRACY'}\n",
    "\n",
    "TRAIN_DATASET_ES=\"./dataset_oppositional/training/dataset_oppositional/dataset_es_train.json\"\n",
    "TRAIN_DATASET_EN=\"./dataset_oppositional/training/dataset_oppositional/dataset_en_train.json\"\n",
    "#TEST_DATASET_EN =\"./dataset_oppositional/test/dataset_oppositional_test_nolabels/dataset_en_official_test_nolabels.json\"\n",
    "#TEST_DATASET_ES =\"./dataset_oppositional/test/dataset_oppositional_test_nolabels/dataset_en_official_test_nolabels.json\"\n",
    "\n",
    "\n",
    "class PAN24Reader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def read_json_file(self, path):\n",
    "        dataset=[]\n",
    "        print(f'Loading official JSON {path} dataset')\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            dataset = json.load(file)\n",
    "        return dataset\n",
    "\n",
    "    def load_dataset_classification(self, path, string_labels=False, positive_class='conspiracy'):\n",
    "        dataset = self.read_json_file(path)\n",
    "        # convert to a format suitable for classification\n",
    "        texts = pd.Series([doc['text'] for doc in dataset])\n",
    "        if string_labels:\n",
    "            classes = pd.Series([doc['category'] for doc in dataset])\n",
    "        else:\n",
    "            if positive_class == 'conspiracy':\n",
    "                binmap = BINARY_MAPPING_CONSPIRACY_POS\n",
    "            elif positive_class == 'critical':\n",
    "                binmap = BINARY_MAPPING_CRITICAL_POS\n",
    "            else:\n",
    "                raise ValueError(f'Unknown positive class: {positive_class}')\n",
    "            classes = [binmap[doc['category']] for doc in dataset]\n",
    "            classes = pd.Series(classes)\n",
    "        ids = pd.Series([doc['id'] for doc in dataset])\n",
    "        data = pd.DataFrame({\"text\": texts, \"id\": ids, \"label\": classes})\n",
    "        return data\n",
    "\n",
    "\n",
    "myReader=PAN24Reader()\n",
    "es_train_df = myReader.load_dataset_classification(TRAIN_DATASET_ES, string_labels=False, positive_class='conspiracy')\n",
    "en_train_df = myReader.load_dataset_classification(TRAIN_DATASET_EN, string_labels=False, positive_class='conspiracy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fine_tunning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(_wandb, _model, _train_data, _val_data, _learning_rate, _optimizer_name, _schedule, _epochs,\n",
    "             _tokenizer, _batch_size=32, _padding=\"max_length\", _max_length=512, _truncation=True,\n",
    "             _patience=10, _measure= \"accuracy\", _out=None):\n",
    "    train_encodings = _tokenizer(_train_data[\"text\"].tolist(), max_length=_max_length, truncation=_truncation, padding=_padding, return_tensors=\"pt\")\n",
    "    val_encodings = _tokenizer(_val_data[\"text\"].tolist(), max_length=_max_length, truncation=_truncation, padding=_padding, return_tensors=\"pt\")\n",
    "\n",
    "    train, val = MyDataset(train_encodings, _train_data[\"label\"].tolist()), MyDataset(val_encodings, _val_data[\"label\"].tolist())\n",
    "\n",
    "    train_dataloader, val_dataloader  = torch.utils.data.DataLoader(train, batch_size=_batch_size, shuffle=True), torch.utils.data.DataLoader(val, batch_size=_batch_size)\n",
    "\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    _wandb.log({\"divice\": str(device)})\n",
    "    if use_cuda:\n",
    "        model = _model.cuda()\n",
    "    best_measure, best_model_name, patience = None, None, 0\n",
    "    training_stats = []\n",
    "    # train_eval = evaluate.load(\"accuracy\")\n",
    "    train_eval = evaluate.load(f\"Yeshwant123/{_measure}\")\n",
    "\n",
    "    lr_scheduler, optimizer = None, None\n",
    "    #Here we can specify different methods to optmize the paarameters, initially we can consider Adam and RmsProp\n",
    "\n",
    "    _wandb.log({\"info\": \"Creating the Optimizer and Schedule \"})\n",
    "\n",
    "    lr_scheduler, optimizer = None, None\n",
    "    if _optimizer_name == \"adam\":\n",
    "        optimizer = Adam(_model.parameters(), lr=_learning_rate)\n",
    "    elif _optimizer_name == \"rmsprop\":\n",
    "        optimizer = RMSprop(_model.parameters(), lr=_learning_rate)\n",
    "\n",
    "    #Here we can define different learning rate schedules, to variate de learning rate in each training step. Initially we use\n",
    "    # can use linear learning rate schedule\n",
    "    num_training_steps = _epochs * len(_train_data)\n",
    "    if _schedule==\"linear\":\n",
    "        lr_scheduler = get_scheduler(_schedule, optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "\n",
    "    for epoch in range(_epochs):\n",
    "        if patience >= _patience: break\n",
    "        total_loss_train, total_acc_train = 0, 0\n",
    "        total_train_step = 0\n",
    "        for batch in train_dataloader:\n",
    "            total_train_step += 1\n",
    "            # print(\"Epoch \", epoch, \"Batch\", i)\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            total_loss_train += loss.item()\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            train_eval.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        total_acc_train = train_eval.compute()\n",
    "\n",
    "        total_eval_steps = 0\n",
    "        total_loss_val, total_acc_val = 0, 0\n",
    "        eval_metric = evaluate.load(f\"Yeshwant123/{_measure}\")\n",
    "        model.eval()\n",
    "        for batch in val_dataloader:\n",
    "            total_eval_steps += 1\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                total_loss_val += loss.item()\n",
    "                logits = outputs.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "            eval_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        total_acc_val = eval_metric.compute()\n",
    "\n",
    "        if best_measure is None or (best_measure < total_acc_val[_measure]):  # here you must set your save weights\n",
    "            if best_measure == None: _wandb.log({\"info\": \"It's the first time (epoch) ******************\"})\n",
    "            elif best_measure < total_acc_val[_measure]:\n",
    "                _wandb.log({\"info\": \"In this epoch an improvement was achieved. (epoch) ******************\"})\n",
    "\n",
    "            best_measure = total_acc_val[_measure]\n",
    "            try:\n",
    "                os.makedirs(_out + os.sep + 'models', exist_ok=True)\n",
    "            except OSError as error:\n",
    "                _wandb.log({\"info\": \"Directory '%s' can not be created\"})\n",
    "            # remove the directories\n",
    "            remove_previous_model(_out + os.sep + 'models')\n",
    "            best_model_name = _out + os.sep + 'models/bestmodel_epoch_{}'.format(epoch +1)\n",
    "            _wandb.log({\"info\": \"The current best model is \" + best_model_name + \" \"+ best_measure})\n",
    "\n",
    "            os.makedirs(best_model_name, exist_ok=True)\n",
    "            model.save_pretrained(best_model_name)\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch + 1,\n",
    "                'Training Loss': total_loss_train / total_train_step,\n",
    "                'Valid. Loss': total_loss_val / total_eval_steps,\n",
    "                f'Valid.{_measure}': total_acc_val[_measure],\n",
    "                f'Training.{_measure}': total_acc_train[_measure]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        _wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': total_loss_train / len(train_dataloader),\n",
    "            f'train_{_measure}': total_acc_train[_measure],\n",
    "            'val_loss': total_loss_val / len(val_dataloader),\n",
    "            f'val_{_measure}': total_acc_val[_measure]\n",
    "        })\n",
    "\n",
    "    if best_model_name != None:\n",
    "        model = model.from_pretrained(best_model_name)\n",
    "        _wandb.log({\"info\": \"The final model used to predict the labels of the testing datasets is \"+ best_model_name})\n",
    "\n",
    "    df_stats = pd.DataFrame(data=training_stats)\n",
    "    df_stats = df_stats.set_index('epoch')\n",
    "    df_stats.to_csv(_out + os.sep + \"training_stats.csv\")\n",
    "\n",
    "    _wandb.log({\"info\": df_stats})\n",
    "    myplot = sns.lineplot(data=df_stats, palette=\"tab10\", linewidth=2.5)\n",
    "    fig = myplot.get_figure()\n",
    "    fig.savefig(_out + os.sep + 'loss-figue.png')\n",
    "    plt.close()\n",
    "    return model\n",
    "\n",
    "############################################################################################################################################################################3\n",
    "#VALIDATION ON THE TEST SET\n",
    "\n",
    "def validate(_wandb, _model, _test_data, _tokenizer, _batch_size=32, _padding=\"max_length\", _max_length=512, _truncation=True, _measure=\"accuracy\", evaltype=True):\n",
    "    test_encodings = _tokenizer(_test_data['text'].tolist(), max_length=_max_length, truncation=_truncation, padding=_padding, return_tensors=\"pt\")\n",
    "    _mode = \"train\" if evaltype else \"test\"\n",
    "    test = MyDataset(test_encodings, _test_data[\"label\"].tolist(), mode=_mode)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=_batch_size)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        model = _model.cuda()\n",
    "    eval_metric, out, k = None, None, 0\n",
    "    if evaltype==True:\n",
    "        eval_metric = evaluate.load(f\"Yeshwant123/{_measure}\")\n",
    "\n",
    "    model.eval()\n",
    "    for batch in test_dataloader:\n",
    "\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "    with torch.no_grad():\n",
    "                outputs = _model(**batch)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                if k == 0:\n",
    "                    out = predictions\n",
    "                else:\n",
    "                    out = torch.cat((out, predictions), 0)\n",
    "                k += 1\n",
    "                total_loss += loss.item()\n",
    "                if evaltype:\n",
    "                    eval_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    if evaltype==True:\n",
    "        total_acc_test = eval_metric.compute()\n",
    "        test_mesure = total_acc_test[_measure]\n",
    "        test_accuarcy = total_acc_test[\"accuracy\"]\n",
    "        avg_test_loss = total_loss / len(test_dataloader)        # Log the test accuracy and loss to wandb\n",
    "        _wandb.log({\n",
    "            f'test_{_measure}': test_mesure,\n",
    "            'test_accuracy': test_accuarcy,\n",
    "            'test_avg_loss': avg_test_loss\n",
    "        })\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
