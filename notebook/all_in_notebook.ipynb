{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dandr\\OneDrive\\1Ciencia de datos\\LNR\\Lab\\oppositional_thinking_transformers\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import evaluate\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam, RMSprop\n",
    "from transformers  import  get_scheduler\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import KFold\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading official JSON ../dataset_oppositional/training/dataset_oppositional/dataset_es_train.json dataset\n",
      "Loading official JSON ../dataset_oppositional/training/dataset_oppositional/dataset_en_train.json dataset\n"
     ]
    }
   ],
   "source": [
    "# utils.py\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Sets the seed to make everything deterministic, for reproducibility of experiments\n",
    "    Parameters:\n",
    "    seed: the number to set the seed to\n",
    "    Return: None\n",
    "    \"\"\"\n",
    "    # Random seed\n",
    "    random.seed(seed)\n",
    "    # Numpy seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Torch seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # os seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def remove_previous_model(folder):\n",
    "    dirs = [x for x in os.listdir(folder) if os.path.isdir(folder+os.sep+x)]\n",
    "    for x in dirs:\n",
    "        shutil.rmtree(folder+os.sep+x, ignore_errors=False, onerror=None)\n",
    "        \n",
    "        \n",
    "def product_dict(**kwargs):\n",
    "    keys = kwargs.keys()\n",
    "    vals = kwargs.values()\n",
    "    for instance in product(*vals):\n",
    "        yield dict(zip(keys, instance))\n",
    "        \n",
    "# mydataset.py\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels, mode='train'):\n",
    "        self.encodings = encodings\n",
    "        if mode !=\"train\":\n",
    "            self.labels=  [0]*len(encodings)\n",
    "        else: self.labels = labels\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "# datareader.py\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "BINARY_MAPPING_CRITICAL_POS = {'CONSPIRACY': 0, 'CRITICAL': 1}\n",
    "BINARY_MAPPING_CONSPIRACY_POS = {'CRITICAL': 0, 'CONSPIRACY': 1}\n",
    "\n",
    "CATEGORY_MAPPING_CRITICAL_POS_INVERSE = {0: 'CONSPIRACY', 1: 'CRITICAL'}\n",
    "CATEGORY_MAPPING_CONSPIRACY_POS_INVERSE = {0: 'CRITICAL', 1: 'CONSPIRACY'}\n",
    "\n",
    "TRAIN_DATASET_ES=\"../dataset_oppositional/training/dataset_oppositional/dataset_es_train.json\"\n",
    "TRAIN_DATASET_EN=\"../dataset_oppositional/training/dataset_oppositional/dataset_en_train.json\"\n",
    "#TEST_DATASET_EN =\"./dataset_oppositional/test/dataset_oppositional_test_nolabels/dataset_en_official_test_nolabels.json\"\n",
    "#TEST_DATASET_ES =\"./dataset_oppositional/test/dataset_oppositional_test_nolabels/dataset_en_official_test_nolabels.json\"\n",
    "\n",
    "\n",
    "class PAN24Reader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def read_json_file(self, path):\n",
    "        dataset=[]\n",
    "        print(f'Loading official JSON {path} dataset')\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            dataset = json.load(file)\n",
    "        return dataset\n",
    "\n",
    "    def load_dataset_classification(self, path, string_labels=False, positive_class='conspiracy'):\n",
    "        dataset = self.read_json_file(path)\n",
    "        # convert to a format suitable for classification\n",
    "        texts = pd.Series([doc['text'] for doc in dataset])\n",
    "        if string_labels:\n",
    "            classes = pd.Series([doc['category'] for doc in dataset])\n",
    "        else:\n",
    "            if positive_class == 'conspiracy':\n",
    "                binmap = BINARY_MAPPING_CONSPIRACY_POS\n",
    "            elif positive_class == 'critical':\n",
    "                binmap = BINARY_MAPPING_CRITICAL_POS\n",
    "            else:\n",
    "                raise ValueError(f'Unknown positive class: {positive_class}')\n",
    "            classes = [binmap[doc['category']] for doc in dataset]\n",
    "            classes = pd.Series(classes)\n",
    "        ids = pd.Series([doc['id'] for doc in dataset])\n",
    "        data = pd.DataFrame({\"text\": texts, \"id\": ids, \"label\": classes})\n",
    "        return data\n",
    "\n",
    "\n",
    "myReader=PAN24Reader()\n",
    "es_train_df = myReader.load_dataset_classification(TRAIN_DATASET_ES, string_labels=False, positive_class='conspiracy')\n",
    "en_train_df = myReader.load_dataset_classification(TRAIN_DATASET_EN, string_labels=False, positive_class='conspiracy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fine_tunning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(_wandb, _model, _train_data, _val_data, _learning_rate, _optimizer_name, _schedule, _epochs,\n",
    "             _tokenizer, _batch_size=32, _padding=\"max_length\", _max_length=512, _truncation=True,\n",
    "             _patience=10, _measure= \"accuracy\", _out=None):\n",
    "    train_encodings = _tokenizer(_train_data[\"text\"].tolist(), max_length=_max_length, truncation=_truncation, padding=_padding, return_tensors=\"pt\")\n",
    "    val_encodings = _tokenizer(_val_data[\"text\"].tolist(), max_length=_max_length, truncation=_truncation, padding=_padding, return_tensors=\"pt\")\n",
    "\n",
    "    train, val = MyDataset(train_encodings, _train_data[\"label\"].tolist()), MyDataset(val_encodings, _val_data[\"label\"].tolist())\n",
    "\n",
    "    train_dataloader, val_dataloader  = torch.utils.data.DataLoader(train, batch_size=_batch_size, shuffle=True), torch.utils.data.DataLoader(val, batch_size=_batch_size)\n",
    "\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    _wandb.log({\"divice\": str(device)})\n",
    "    if use_cuda:\n",
    "        model = _model.cuda()\n",
    "    best_measure, best_model_name, patience = None, None, 0\n",
    "    training_stats = []\n",
    "    # train_eval = evaluate.load(\"accuracy\")\n",
    "    train_eval = evaluate.load(f\"Yeshwant123/{_measure}\")\n",
    "\n",
    "    lr_scheduler, optimizer = None, None\n",
    "    #Here we can specify different methods to optmize the paarameters, initially we can consider Adam and RmsProp\n",
    "\n",
    "    _wandb.log({\"info\": \"Creating the Optimizer and Schedule \"})\n",
    "\n",
    "    lr_scheduler, optimizer = None, None\n",
    "    if _optimizer_name == \"adam\":\n",
    "        optimizer = Adam(_model.parameters(), lr=_learning_rate)\n",
    "    elif _optimizer_name == \"rmsprop\":\n",
    "        optimizer = RMSprop(_model.parameters(), lr=_learning_rate)\n",
    "\n",
    "    #Here we can define different learning rate schedules, to variate de learning rate in each training step. Initially we use\n",
    "    # can use linear learning rate schedule\n",
    "    num_training_steps = _epochs * len(_train_data)\n",
    "    if _schedule==\"linear\":\n",
    "        lr_scheduler = get_scheduler(_schedule, optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "\n",
    "    for epoch in range(_epochs):\n",
    "        if patience >= _patience: break\n",
    "        total_loss_train, total_acc_train = 0, 0\n",
    "        total_train_step = 0\n",
    "        for batch in train_dataloader:\n",
    "            total_train_step += 1\n",
    "            # print(\"Epoch \", epoch, \"Batch\", i)\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            total_loss_train += loss.item()\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            train_eval.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        total_acc_train = train_eval.compute()\n",
    "\n",
    "        total_eval_steps = 0\n",
    "        total_loss_val, total_acc_val = 0, 0\n",
    "        eval_metric = evaluate.load(f\"Yeshwant123/{_measure}\")\n",
    "        model.eval()\n",
    "        for batch in val_dataloader:\n",
    "            total_eval_steps += 1\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                total_loss_val += loss.item()\n",
    "                logits = outputs.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "            eval_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        total_acc_val = eval_metric.compute()\n",
    "\n",
    "        if best_measure is None or (best_measure < total_acc_val[_measure]):  # here you must set your save weights\n",
    "            if best_measure == None: _wandb.log({\"info\": \"It's the first time (epoch) ******************\"})\n",
    "            elif best_measure < total_acc_val[_measure]:\n",
    "                _wandb.log({\"info\": \"In this epoch an improvement was achieved. (epoch) ******************\"})\n",
    "\n",
    "            best_measure = total_acc_val[_measure]\n",
    "            try:\n",
    "                os.makedirs(_out + os.sep + 'models', exist_ok=True)\n",
    "            except OSError as error:\n",
    "                _wandb.log({\"info\": \"Directory '%s' can not be created\"})\n",
    "            # remove the directories\n",
    "            remove_previous_model(_out + os.sep + 'models')\n",
    "            best_model_name = _out + os.sep + 'models/bestmodel_epoch_{}'.format(epoch +1)\n",
    "            _wandb.log({\"info\": \"The current best model is \" + best_model_name + \" \"+ best_measure})\n",
    "\n",
    "            os.makedirs(best_model_name, exist_ok=True)\n",
    "            model.save_pretrained(best_model_name)\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch + 1,\n",
    "                'Training Loss': total_loss_train / total_train_step,\n",
    "                'Valid. Loss': total_loss_val / total_eval_steps,\n",
    "                f'Valid.{_measure}': total_acc_val[_measure],\n",
    "                f'Training.{_measure}': total_acc_train[_measure]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        _wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': total_loss_train / len(train_dataloader),\n",
    "            f'train_{_measure}': total_acc_train[_measure],\n",
    "            'val_loss': total_loss_val / len(val_dataloader),\n",
    "            f'val_{_measure}': total_acc_val[_measure]\n",
    "        })\n",
    "\n",
    "    if best_model_name != None:\n",
    "        model = model.from_pretrained(best_model_name)\n",
    "        _wandb.log({\"info\": \"The final model used to predict the labels of the testing datasets is \"+ best_model_name})\n",
    "\n",
    "    df_stats = pd.DataFrame(data=training_stats)\n",
    "    df_stats = df_stats.set_index('epoch')\n",
    "    df_stats.to_csv(_out + os.sep + \"training_stats.csv\")\n",
    "\n",
    "    _wandb.log({\"info\": df_stats})\n",
    "    myplot = sns.lineplot(data=df_stats, palette=\"tab10\", linewidth=2.5)\n",
    "    fig = myplot.get_figure()\n",
    "    fig.savefig(_out + os.sep + 'loss-figue.png')\n",
    "    plt.close()\n",
    "    return model\n",
    "\n",
    "############################################################################################################################################################################3\n",
    "#VALIDATION ON THE TEST SET\n",
    "\n",
    "def validate(_wandb, _model, _test_data, _tokenizer, _batch_size=32, _padding=\"max_length\", _max_length=512, _truncation=True, _measure=\"accuracy\", evaltype=True):\n",
    "    test_encodings = _tokenizer(_test_data['text'].tolist(), max_length=_max_length, truncation=_truncation, padding=_padding, return_tensors=\"pt\")\n",
    "    _mode = \"train\" if evaltype else \"test\"\n",
    "    test = MyDataset(test_encodings, _test_data[\"label\"].tolist(), mode=_mode)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=_batch_size)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        model = _model.cuda()\n",
    "    eval_metric, out, k = None, None, 0\n",
    "    if evaltype==True:\n",
    "        eval_metric = evaluate.load(f\"Yeshwant123/{_measure}\")\n",
    "\n",
    "    model.eval()\n",
    "    for batch in test_dataloader:\n",
    "\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "    with torch.no_grad():\n",
    "                outputs = _model(**batch)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                if k == 0:\n",
    "                    out = predictions\n",
    "                else:\n",
    "                    out = torch.cat((out, predictions), 0)\n",
    "                k += 1\n",
    "                total_loss += loss.item()\n",
    "                if evaltype:\n",
    "                    eval_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    if evaltype==True:\n",
    "        total_acc_test = eval_metric.compute()\n",
    "        test_mesure = total_acc_test[_measure]\n",
    "        test_accuarcy = total_acc_test[\"accuracy\"]\n",
    "        avg_test_loss = total_loss / len(test_dataloader)        # Log the test accuracy and loss to wandb\n",
    "        _wandb.log({\n",
    "            f'test_{_measure}': test_mesure,\n",
    "            'test_accuracy': test_accuarcy,\n",
    "            'test_avg_loss': avg_test_loss\n",
    "        })\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdavidandreuroqueta\u001b[0m (\u001b[33mdavidandreu-org\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer roberta-base\n",
      "Loading Transformer Model roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdavidandreuroqueta\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\dandr\\OneDrive\\1Ciencia de datos\\LNR\\Lab\\oppositional_thinking_transformers\\notebook\\wandb\\run-20240519_201923-mvui7scq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-19_20-19-17/runs/mvui7scq' target=\"_blank\">english_roberta-base_1_fold_0</a></strong> to <a href='https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-19_20-19-17' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-19_20-19-17' target=\"_blank\">https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-19_20-19-17</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-19_20-19-17/runs/mvui7scq' target=\"_blank\">https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-19_20-19-17/runs/mvui7scq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Format it to include hours, minutes, and seconds\n",
    "formatted_datetime = current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "SEED=1234\n",
    "set_seed(SEED)\n",
    "\n",
    "preconfig = {\n",
    "    0: {\n",
    "        \"lang\": \"english\",\n",
    "        \"model_name\": \"roberta-base\",\n",
    "    },\n",
    "    1: {\n",
    "        \"lang\": \"english\",\n",
    "        \"model_name\": \"microsoft/deberta-base\",\n",
    "    },\n",
    "    # 2: {\n",
    "    #     \"lang\": \"spanish\",\n",
    "    #     \"model_name\": \"dccuchile/bert-base-spanish-wwm-uncased\",\n",
    "    # },\n",
    "    # 3: {\n",
    "    #     \"lang\": \"spanish\",\n",
    "    #     \"model_name\": \"PlanTL-GOB-ES/roberta-base-bne\",\n",
    "    # },\n",
    "    # 4: {\n",
    "    #     \"lang\": \"spanish\",\n",
    "    #     \"model_name\": \"bert-base-multilingual-uncased\"\n",
    "    # }\n",
    "}\n",
    "\n",
    "hyperparams = {\n",
    "    \"optimizer_name\": [\"adam\", \"rmsprop\"], # [\"adam\", \"rmsprop\", \"sgd\"]\n",
    "    \"learning\": [0.5e-5, 1e-6], # [0.5e-5, 1e-5, 0.5e-6, 1e-6\n",
    "    \"schedule\": [\"linear\", \"cosine\"], # [\"linear\", \"cosine\", \"constant\"]\n",
    "    \"patience\": [5, 10], # [3, 5, 10]\n",
    "    \"epochs\": [5, 20], # [5, 10, 20]\n",
    "    \"measure\": [\"mcc\"],\n",
    "    \"batch_size\": [32], # [16, 32, 64, 128]\n",
    "    \"max_length\": [128]\n",
    "}\n",
    "\n",
    "# epochs = 5 #[5, 10, 20]\n",
    "# batch_size = 32 #[16, 32, 64, 128]\n",
    "# measure = \"mcc\"\n",
    "# patience = 3 #[5, 10]\n",
    "# max_length = 128 #[This value can be estimated on the training set]\n",
    "# Define KFold cross-validation\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# For each preconfiguration\n",
    "for i, preconfig in preconfig.items():\n",
    "    lang = preconfig[\"lang\"]\n",
    "    model_name = preconfig[\"model_name\"]\n",
    "    \n",
    "    if lang == \"spanish\":\n",
    "        X= es_train_df\n",
    "    elif lang == \"english\":\n",
    "        X= en_train_df\n",
    "    \n",
    "    # Start a parent run for this preconfiguration\n",
    "    # parent_run = wandb.init(project='lnr_oppositional_thinking',\n",
    "    #                         entity='davidandreuroqueta',\n",
    "    #                         group=f'{lang}_{model_name}',\n",
    "    #                         job_type='model')\n",
    "    # parent_run.config.update(preconfig)\n",
    "    # parent_run.config.update({\"SEED\":SEED})\n",
    "\n",
    "    print(\"Loading Tokenizer \" + model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(\"Loading Transformer Model \" + model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "\n",
    "    # Initialize a counter for the runs\n",
    "    run_counter = 0\n",
    "\n",
    "    # For each hyperparameter configuration\n",
    "    for config in product_dict(**hyperparams):\n",
    "        run_counter += 1\n",
    "        # Start a child run for this hyperparameter configuration\n",
    "        # with wandb.init(project='lnr_oppositional_thinking',\n",
    "        #                 entity='davidandreuroqueta',\n",
    "        #                 group=f'{lang}_{model_name}',\n",
    "        #                 job_type='hyperparam-tuning',\n",
    "        #                 name=f'{lang}_{model_name}_{run_counter}',\n",
    "        #                 ) as run:\n",
    "        #     # Log hyperparameters\n",
    "        #     run.config.update(config)\n",
    "            \n",
    "        # For each fold\n",
    "        for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "            X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "\n",
    "            # Start a child run for this fold\n",
    "            with wandb.init(project=f'lnr_oppositional_thinking_{formatted_datetime}',\n",
    "                            entity='davidandreuroqueta',\n",
    "                            group=f'{lang}_{model_name}',\n",
    "                            job_type=f'hyperparam-tuning-{run_counter}',\n",
    "                            name=f'{lang}_{model_name}_{run_counter}_fold_{fold}'\n",
    "                            ) as fold_run:\n",
    "                fold_run.config.update(preconfig)\n",
    "                fold_run.config.update(config)\n",
    "                fold_run.config.update({\"SEED\":SEED})\n",
    "\n",
    "                # Log the fold number\n",
    "                fold_run.config.update({\"fold\": fold + 1})\n",
    "\n",
    "                # Train and validate your model, log metrics, etc.\n",
    "                # ...\n",
    "                # FINE-TUNING the model and obtaining the best model across all epochs\n",
    "                fineTmodel = training(_wandb=fold_run, _model=model, _train_data=X_train, _val_data=X_val,\n",
    "                                    _learning_rate=config[\"learning\"], _optimizer_name=config[\"optimizer_name\"],\n",
    "                                    _schedule=config[\"schedule\"], _epochs=config[\"epochs\"], _tokenizer=tokenizer,\n",
    "                                    _batch_size=config[\"batch_size\"], _padding=\"max_length\", _max_length=config[\"max_length\"],\n",
    "                                    _truncation=True, _patience=config[\"patience\"], _measure=config[\"measure\"], _out=\"./out\")\n",
    "\n",
    "                # VALIDATING OR PREDICTING on the test partition, this time I'm using the validation set, but you have to use the test set.\n",
    "                preds = validate(_wandb=fold_run, _model=fineTmodel, _test_data=X_val, _tokenizer=tokenizer,\n",
    "                                _batch_size=config[\"batch_size\"], _padding=\"max_length\", _max_length=config[\"max_length\"],\n",
    "                                _truncation=True, _measure=config[\"measure\"], evaltype=True)\n",
    "\n",
    "    # End the parent run\n",
    "    # parent_run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
