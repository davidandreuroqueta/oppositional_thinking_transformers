{"cells":[{"cell_type":"markdown","metadata":{"id":"cSIZe_0eJGt4"},"source":["install libraries"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-27T08:00:13.487487Z","iopub.status.busy":"2024-05-27T08:00:13.486751Z","iopub.status.idle":"2024-05-27T08:00:13.491457Z","shell.execute_reply":"2024-05-27T08:00:13.490410Z","shell.execute_reply.started":"2024-05-27T08:00:13.487454Z"},"id":"L87-IBWhJOKv","outputId":"15da57b8-89f5-493c-848b-4a8b79181e59","trusted":true},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('Oppositional_thinking')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-27T14:02:23.061637Z","iopub.status.busy":"2024-05-27T14:02:23.060884Z","iopub.status.idle":"2024-05-27T14:02:36.813783Z","shell.execute_reply":"2024-05-27T14:02:36.812630Z","shell.execute_reply.started":"2024-05-27T14:02:23.061576Z"},"id":"6CqTVHUxJGuB","outputId":"e08111ca-75d0-41ee-e364-fb50309d4c8c","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Collecting evaluate\n","  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n","Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.18.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.22.2)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n","Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n","\u001b[?25hInstalling collected packages: evaluate\n","Successfully installed evaluate-0.4.2\n"]}],"source":["!pip install evaluate"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-27T14:02:36.816685Z","iopub.status.busy":"2024-05-27T14:02:36.816257Z","iopub.status.idle":"2024-05-27T14:02:36.896202Z","shell.execute_reply":"2024-05-27T14:02:36.895195Z","shell.execute_reply.started":"2024-05-27T14:02:36.816645Z"},"id":"EmTP5PP2K0GA","outputId":"d9c16f3f-16bc-4895-cdac-30aa5012aab4","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]},{"data":{"text/plain":["2"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# test gpu:\n","import torch\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","print(device)\n","torch.cuda.device_count()"]},{"cell_type":"markdown","metadata":{"id":"YI4-aP4wJGuF"},"source":[]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T14:02:46.772538Z","iopub.status.busy":"2024-05-27T14:02:46.772192Z","iopub.status.idle":"2024-05-27T14:02:46.804602Z","shell.execute_reply":"2024-05-27T14:02:46.803748Z","shell.execute_reply.started":"2024-05-27T14:02:46.772506Z"},"id":"HfvFoGVnJGuG","trusted":true},"outputs":[],"source":["import os\n","import torch\n","import evaluate\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from tqdm import tqdm\n","from torch.optim import Adam, RMSprop\n","from transformers  import  get_scheduler\n","from sklearn.model_selection import KFold\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from sklearn.model_selection import KFold\n","import wandb\n","from datetime import datetime\n","import os\n","import shutil\n","import random\n","import numpy as np\n","from itertools import product\n"]},{"cell_type":"markdown","metadata":{"id":"hahMpZ91JGuM"},"source":["utils"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-27T14:02:46.806223Z","iopub.status.busy":"2024-05-27T14:02:46.805853Z","iopub.status.idle":"2024-05-27T14:02:47.738278Z","shell.execute_reply":"2024-05-27T14:02:47.737527Z","shell.execute_reply.started":"2024-05-27T14:02:46.806193Z"},"id":"hYGZxU7UJGuN","outputId":"fbd7ee10-e8b4-48de-e4f7-02841bdd8ad4","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading official JSON /kaggle/input/dataset-oppositional/dataset_oppositional/training/dataset_oppositional/dataset_es_train.json dataset\n","Loading official JSON /kaggle/input/dataset-oppositional/dataset_oppositional/training/dataset_oppositional/dataset_en_train.json dataset\n"]}],"source":["# utils.py\n","def set_seed(seed):\n","    \"\"\"\n","    Sets the seed to make everything deterministic, for reproducibility of experiments\n","    Parameters:\n","    seed: the number to set the seed to\n","    Return: None\n","    \"\"\"\n","    # Random seed\n","    random.seed(seed)\n","    # Numpy seed\n","    np.random.seed(seed)\n","\n","    # Torch seed\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","    # os seed\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","\n","def remove_previous_model(folder):\n","    dirs = [x for x in os.listdir(folder) if os.path.isdir(folder+os.sep+x)]\n","    for x in dirs:\n","        shutil.rmtree(folder+os.sep+x, ignore_errors=False, onerror=None)\n","\n","\n","def product_dict(**kwargs):\n","    keys = kwargs.keys()\n","    vals = kwargs.values()\n","    for instance in product(*vals):\n","        yield dict(zip(keys, instance))\n","\n","# mydataset.py\n","class MyDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels, mode='train'):\n","        self.encodings = encodings\n","        if mode !=\"train\":\n","            self.labels=  [0]*len(encodings)\n","        else: self.labels = labels\n","\n","\n","    def __getitem__(self, idx):\n","        # item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx]).clone().detach()\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","# datareader.py\n","import pandas as pd\n","import json\n","\n","BINARY_MAPPING_CRITICAL_POS = {'CONSPIRACY': 0, 'CRITICAL': 1}\n","BINARY_MAPPING_CONSPIRACY_POS = {'CRITICAL': 0, 'CONSPIRACY': 1}\n","\n","CATEGORY_MAPPING_CRITICAL_POS_INVERSE = {0: 'CONSPIRACY', 1: 'CRITICAL'}\n","CATEGORY_MAPPING_CONSPIRACY_POS_INVERSE = {0: 'CRITICAL', 1: 'CONSPIRACY'}\n","\n","TRAIN_DATASET_ES=\"/kaggle/input/dataset-oppositional/dataset_oppositional/training/dataset_oppositional/dataset_es_train.json\"\n","TRAIN_DATASET_EN=\"/kaggle/input/dataset-oppositional/dataset_oppositional/training/dataset_oppositional/dataset_en_train.json\"\n","#TEST_DATASET_EN =\"./dataset_oppositional/test/dataset_oppositional_test_nolabels/dataset_en_official_test_nolabels.json\"\n","#TEST_DATASET_ES =\"./dataset_oppositional/test/dataset_oppositional_test_nolabels/dataset_en_official_test_nolabels.json\"\n","\n","\n","class PAN24Reader:\n","    def __init__(self):\n","        pass\n","    def read_json_file(self, path):\n","        dataset=[]\n","        print(f'Loading official JSON {path} dataset')\n","        with open(path, 'r', encoding='utf-8') as file:\n","            dataset = json.load(file)\n","        return dataset\n","\n","    def load_dataset_classification(self, path, string_labels=False, positive_class='conspiracy'):\n","        dataset = self.read_json_file(path)\n","        # convert to a format suitable for classification\n","        texts = pd.Series([doc['text'] for doc in dataset])\n","        if string_labels:\n","            classes = pd.Series([doc['category'] for doc in dataset])\n","        else:\n","            if positive_class == 'conspiracy':\n","                binmap = BINARY_MAPPING_CONSPIRACY_POS\n","            elif positive_class == 'critical':\n","                binmap = BINARY_MAPPING_CRITICAL_POS\n","            else:\n","                raise ValueError(f'Unknown positive class: {positive_class}')\n","            classes = [binmap[doc['category']] for doc in dataset]\n","            classes = pd.Series(classes)\n","        ids = pd.Series([doc['id'] for doc in dataset])\n","        data = pd.DataFrame({\"text\": texts, \"id\": ids, \"label\": classes})\n","        return data\n","\n","\n","myReader=PAN24Reader()\n","es_train_df = myReader.load_dataset_classification(TRAIN_DATASET_ES, string_labels=False, positive_class='conspiracy')\n","en_train_df = myReader.load_dataset_classification(TRAIN_DATASET_EN, string_labels=False, positive_class='conspiracy')\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LHeNe9N1JGuP"},"source":["fine_tunning.py"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T14:02:51.777890Z","iopub.status.busy":"2024-05-27T14:02:51.777495Z","iopub.status.idle":"2024-05-27T14:02:51.815120Z","shell.execute_reply":"2024-05-27T14:02:51.814167Z","shell.execute_reply.started":"2024-05-27T14:02:51.777865Z"},"id":"T4EmKT7PJGuQ","trusted":true},"outputs":[],"source":["def training(_wandb, _model, _train_data, _val_data, _learning_rate, _optimizer_name, _schedule, _epochs,\n","             _tokenizer, _batch_size=32, _padding=\"max_length\", _max_length=512, _truncation=True,\n","             _patience=10, _measure= \"accuracy\", _out=None):\n","    train_encodings = _tokenizer(_train_data[\"text\"].tolist(), max_length=_max_length, truncation=_truncation, padding=_padding, return_tensors=\"pt\")\n","    val_encodings = _tokenizer(_val_data[\"text\"].tolist(), max_length=_max_length, truncation=_truncation, padding=_padding, return_tensors=\"pt\")\n","\n","    train, val = MyDataset(train_encodings, _train_data[\"label\"].tolist()), MyDataset(val_encodings, _val_data[\"label\"].tolist())\n","\n","    train_dataloader, val_dataloader  = torch.utils.data.DataLoader(train, batch_size=_batch_size, shuffle=True), torch.utils.data.DataLoader(val, batch_size=_batch_size)\n","\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","    _wandb.log({\"divice\": str(device)})\n","    if use_cuda:\n","        model = _model.to(device)\n","        if torch.cuda.device_count() > 1:\n","            print(f\"Usando {torch.cuda.device_count()} GPUs\")\n","            model = torch.nn.DataParallel(model)\n","    best_measure, best_model_name, patience = None, None, 0\n","    training_stats = []\n","    \n","    # train_eval = evaluate.load(\"accuracy\")\n","    train_eval = evaluate.load(f\"Yeshwant123/{_measure}\")\n","\n","    lr_scheduler, optimizer = None, None\n","    #Here we can specify different methods to optmize the paarameters, initially we can consider Adam and RmsProp\n","\n","    _wandb.log({\"info\": \"Creating the Optimizer and Schedule \"})\n","\n","    lr_scheduler, optimizer = None, None\n","    if _optimizer_name == \"adam\":\n","        optimizer = Adam(_model.parameters(), lr=_learning_rate)\n","    elif _optimizer_name == \"rmsprop\":\n","        optimizer = RMSprop(_model.parameters(), lr=_learning_rate)\n","\n","    #Here we can define different learning rate schedules, to variate de learning rate in each training step. Initially we use\n","    # can use linear learning rate schedule\n","    num_training_steps = _epochs * len(_train_data)\n","    if _schedule==\"linear\":\n","        lr_scheduler = get_scheduler(_schedule, optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","\n","    for epoch in range(_epochs):\n","        if patience >= _patience: break\n","        total_loss_train, total_acc_train = 0, 0\n","        total_train_step = 0\n","        \n","        _model.train()\n","        \n","        for batch in train_dataloader:\n","            total_train_step += 1\n","            # print(\"Epoch \", epoch, \"Batch\", i)\n","            batch = {k: v.to(device) for k, v in batch.items()}\n","            outputs = model(**batch)\n","            loss = outputs.loss\n","            if loss.dim() > 0:\n","                loss = loss.mean()  # Ensure the loss is a scalar\n","            loss.backward()\n","            total_loss_train += loss.item()\n","            logits = outputs.logits\n","            predictions = torch.argmax(logits, dim=-1)\n","            train_eval.add_batch(predictions=predictions, references=batch[\"labels\"])\n","            optimizer.step()\n","            lr_scheduler.step()\n","            optimizer.zero_grad()\n","        total_acc_train = train_eval.compute()\n","\n","        total_eval_steps = 0\n","        total_loss_val, total_acc_val = 0, 0\n","        eval_metric = evaluate.load(f\"Yeshwant123/{_measure}\")\n","        model.eval()\n","        for batch in val_dataloader:\n","            total_eval_steps += 1\n","            batch = {k: v.to(device) for k, v in batch.items()}\n","            with torch.no_grad():\n","                outputs = model(**batch)\n","                loss = outputs.loss\n","                if loss.dim() > 0:\n","                    loss = loss.mean()  # Ensure the loss is a scalar\n","                total_loss_val += loss.item()\n","                logits = outputs.logits\n","                predictions = torch.argmax(logits, dim=-1)\n","            eval_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n","        total_acc_val = eval_metric.compute()\n","\n","        if best_measure is None or (best_measure < total_acc_val[_measure]):  # here you must set your save weights\n","            if best_measure == None: _wandb.log({\"info\": \"It's the first time (epoch) ******************\"})\n","            elif best_measure < total_acc_val[_measure]:\n","                _wandb.log({\"info\": \"In this epoch an improvement was achieved. (epoch) ******************\"})\n","\n","            best_measure = total_acc_val[_measure]\n","            try:\n","                os.makedirs(_out + os.sep + 'models', exist_ok=True)\n","            except OSError as error:\n","                _wandb.log({\"info\": \"Directory '%s' can not be created\"})\n","            # remove the directories\n","            remove_previous_model(_out + os.sep + 'models')\n","            best_model_name = _out + os.sep + 'models/bestmodel_epoch_{}'.format(epoch +1)\n","            _wandb.log({\"info\": \"The current best model is \" + best_model_name + \" \"+ str(best_measure)})\n","\n","            os.makedirs(best_model_name, exist_ok=True)\n","            if isinstance(model, torch.nn.DataParallel):\n","                model.module.save_pretrained(best_model_name)\n","            else:\n","                model.save_pretrained(best_model_name)\n","            patience = 0\n","        else:\n","            patience += 1\n","        training_stats.append(\n","            {\n","                'epoch': epoch + 1,\n","                'Training Loss': total_loss_train / total_train_step,\n","                'Valid. Loss': total_loss_val / total_eval_steps,\n","                f'Valid.{_measure}': total_acc_val[_measure],\n","                f'Training.{_measure}': total_acc_train[_measure]\n","            }\n","        )\n","        \n","        _wandb.log({\n","            'epoch': epoch + 1,\n","            'train_loss': total_loss_train / len(train_dataloader),\n","            f'train_{_measure}': total_acc_train[_measure],\n","            'val_loss': total_loss_val / len(val_dataloader),\n","            f'val_{_measure}': total_acc_val[_measure]\n","        })\n","\n","    if best_model_name != None:\n","        if isinstance(model, torch.nn.DataParallel):\n","            model = model.module.from_pretrained(best_model_name)\n","        else:\n","            model = model.from_pretrained(best_model_name)\n","        _wandb.log({\"info\": \"The final model used to predict the labels of the testing datasets is \" + best_model_name})\n","\n","    df_stats = pd.DataFrame(data=training_stats)\n","    df_stats = df_stats.set_index('epoch')\n","    df_stats.to_csv(_out + os.sep + \"training_stats.csv\")\n","\n","    _wandb.log({\"info\": df_stats})\n","    myplot = sns.lineplot(data=df_stats, palette=\"tab10\", linewidth=2.5)\n","    fig = myplot.get_figure()\n","    fig.savefig(_out + os.sep + 'loss-figue.png')\n","    plt.close()\n","    return model\n","\n","############################################################################################################################################################################3\n","#VALIDATION ON THE TEST SET\n","\n","def validate(_wandb, _model, _test_data, _tokenizer, _batch_size=32, _padding=\"max_length\", _max_length=512, _truncation=True, _measure=\"accuracy\", evaltype=True):\n","    test_encodings = _tokenizer(_test_data['text'].tolist(), max_length=_max_length, truncation=_truncation, padding=_padding, return_tensors=\"pt\")\n","    _mode = \"train\" if evaltype else \"test\"\n","    test = MyDataset(test_encodings, _test_data[\"label\"].tolist(), mode=_mode)\n","    test_dataloader = torch.utils.data.DataLoader(test, batch_size=_batch_size)\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","    if use_cuda:\n","        model = _model.to(device)\n","        if torch.cuda.device_count() > 1:\n","            print(f\"Usando {torch.cuda.device_count()} GPUs\")\n","            model = torch.nn.DataParallel(model)\n","    \n","    eval_metric, out, k = None, None, 0\n","\n","    if evaltype==True:\n","        eval_metric = evaluate.load(f\"Yeshwant123/{_measure}\")\n","\n","    model.eval()\n","    total_loss = 0\n","\n","    for batch in test_dataloader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","\n","    with torch.no_grad():\n","                outputs = _model(**batch)\n","                loss = outputs.loss\n","                if loss.dim() > 0:\n","                    loss = loss.mean()  # Ensure the loss is a scalar\n","                logits = outputs.logits\n","                predictions = torch.argmax(logits, dim=-1)\n","                if k == 0:\n","                    out = predictions\n","                else:\n","                    out = torch.cat((out, predictions), 0)\n","                k += 1\n","                total_loss += loss.item()\n","                if evaltype:\n","                    eval_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n","\n","    if evaltype==True:\n","        total_acc_test = eval_metric.compute()\n","        test_mesure = total_acc_test[_measure]\n","        avg_test_loss = total_loss / len(test_dataloader)        # Log the test accuracy and loss to wandb\n","        _wandb.log({\n","            f'test_{_measure}': test_mesure,\n","            'test_avg_loss': avg_test_loss\n","        })\n","    return out\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T14:30:47.356971Z","iopub.status.busy":"2024-05-27T14:30:47.356330Z","iopub.status.idle":"2024-05-27T14:30:47.361704Z","shell.execute_reply":"2024-05-27T14:30:47.360676Z","shell.execute_reply.started":"2024-05-27T14:30:47.356937Z"},"trusted":true},"outputs":[],"source":["# hyperparams_covered = [\n","#     {\n","#         \"optimizer_name\": \"adam\", \n","#         \"learning\": 0.5e-5, \n","#         \"schedule\": \"linear\",\n","#         \"patience\": 5, \n","#         \"epochs\": 5,\n","#         \"measure\": \"mcc\",\n","#         \"batch_size\": 32, \n","#         \"max_length\": 128\n","#     },\n","#     {\n","#         \"optimizer_name\": \"adam\", \n","#         \"learning\": 0.5e-5, \n","#         \"schedule\": \"linear\",\n","#         \"patience\": 5, \n","#         \"epochs\": 10,\n","#         \"measure\": \"mcc\",\n","#         \"batch_size\": 32, \n","#         \"max_length\": 128\n","#     },\n","# ]\n","hyperparams_covered = []"]},{"cell_type":"markdown","metadata":{"id":"nVzhkZtwJGuT"},"source":["main.py"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["42d5bd90622e4626b8f73ab130cf846f","a027348f0aaf4a3bb4e354e9b73bd8ef","ffdd2c63a036471ebf0692a844fc12d8","f3c6d847ab604a57aa04ab3f2ba258e2","30e03ce93eb34293bcd87ca7959854a3","7921f8a3b21645d9afded2d1ef2b59ba","9ce3a42132254f7ea112342a71a658c6","eb5dc2642e2345e0a5582a4f94451389","ef4721235ee240d08badd81528eaf79c","aa8399df41354082b2923f1e99075ed9","3407d57d8488404eaff1306a93b3b4ac","7a5d4ee7078b4f67965c4ccb6b793625","3f0b92538fc04fd08cd2e1572de89f84","b71b0e1ce144434c9fc9b4d9f781997c","b822b37bb5cd40e58f423e83a82a61e2","93271ff93bec44f583c1a4622529f1ce","b48d92ae81684a96be46844484e156fb","e7a687efc09840a280c42bf8f3d5e4a5","15bad4d6a1164c70acbc8db7edbd9e66","21f30a65139d4aa6b755660371049a03","7cedcf6632e643b383e8d9c196af693a","ccd6079762884df5b2ec6a5ffb5a47bc","62607de3071e4999951dd21f2d7422c4","929d8dcca0024332b37e7e5c4b6ef00e","3946ec0b82444435bbef194b589ce92d","7551706e14434d37885ece328004c5ec","e8cff45d1e774b3bab4dd6befb10d43c","e67937bd65224880a199beffec88be89","7c987fd2de8b405a8d294975214cbc3d","7092d9f13ee9493f95362b26e23f601e","13e22342799a4323bf3f1059e082827f","e18ed07e2eea4fce8dd7a06d5edc5317"]},"execution":{"iopub.execute_input":"2024-05-27T08:00:26.788708Z","iopub.status.busy":"2024-05-27T08:00:26.788377Z","iopub.status.idle":"2024-05-27T08:00:26.799022Z","shell.execute_reply":"2024-05-27T08:00:26.798173Z","shell.execute_reply.started":"2024-05-27T08:00:26.788679Z"},"id":"Tin2G53rJGuU","outputId":"2d956185-59c7-467a-9d83-d915fa7d2013","trusted":true},"outputs":[],"source":["# # Get current date and time\n","# current_datetime = datetime.now()\n","\n","# # Format it to include hours, minutes, and seconds\n","# formatted_datetime = current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n","\n","# settings = wandb.Settings(start_method='thread', console='auto', mode='online')\n","\n","\n","# wandb.login()\n","\n","# SEED=1234\n","# set_seed(SEED)\n","\n","# preconfig = {\n","#     0: {\n","#         \"lang\": \"english\",\n","#         \"model_name\": \"roberta-base\",\n","#     },\n","#     1: {\n","#         \"lang\": \"english\",\n","#         \"model_name\": \"microsoft/deberta-base\",\n","#     },\n","#     # 2: {\n","#     #     \"lang\": \"spanish\",\n","#     #     \"model_name\": \"dccuchile/bert-base-spanish-wwm-uncased\",\n","#     # },\n","#     # 3: {\n","#     #     \"lang\": \"spanish\",\n","#     #     \"model_name\": \"PlanTL-GOB-ES/roberta-base-bne\",\n","#     # },\n","#     # 4: {\n","#     #     \"lang\": \"spanish\",\n","#     #     \"model_name\": \"bert-base-multilingual-uncased\"\n","#     # }\n","# }\n","\n","# hyperparams = {\n","#     \"optimizer_name\": [\"adam\", \"rmsprop\"], # [\"adam\", \"rmsprop\", \"sgd\"]\n","#     \"learning\": [0.5e-5, 1e-6], # [0.5e-5, 1e-5, 0.5e-6, 1e-6\n","#     \"schedule\": [\"linear\", \"cosine\"], # [\"linear\", \"cosine\", \"constant\"]\n","#     \"patience\": [5, 10], # [3, 5, 10]\n","#     \"epochs\": [5, 10], # [5, 10, 20]\n","#     \"measure\": [\"mcc\"],\n","#     \"batch_size\": [64], # [16, 32, 64, 128]\n","#     \"max_length\": [128]\n","# }\n","\n","# # Define KFold cross-validation\n","# kf = KFold(n_splits=5)\n","\n","# # For each preconfiguration\n","# for i, preconfig in preconfig.items():\n","#     lang = preconfig[\"lang\"]\n","#     model_name = preconfig[\"model_name\"]\n","    \n","#     if lang == \"spanish\":\n","#         X= es_train_df\n","#     elif lang == \"english\":\n","#         X= en_train_df\n","    \n","#     print(\"Loading Tokenizer \" + model_name)\n","#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","#     # Initialize a counter for the runs\n","#     run_counter = 0\n","\n","#     # For each hyperparameter configuration\n","#     for config in product_dict(**hyperparams):\n","#         run_counter += 1\n","#         if config in hyperparams_covered:\n","#             continue\n","#         print(f\"Iniciando proceso con los hiperparametros:\\n{config}\")\n","            \n","#         # For each fold\n","#         for fold, (train_index, val_index) in enumerate(kf.split(X)):\n","#             X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n","\n","#             # Start a child run for this fold\n","#             with wandb.init(project=f'lnr_oppositional_thinking_{formatted_datetime}',\n","#                             entity='davidandreuroqueta',\n","#                             group=f'{lang}_{model_name}',\n","#                             job_type=f'hyperparam-tuning-{run_counter}',\n","#                             name=f'{lang}_{model_name}_{run_counter}_fold_{fold}',\n","#                             settings=settings) as fold_run:\n","#                 fold_run.config.update(preconfig)\n","#                 fold_run.config.update(config)\n","#                 fold_run.config.update({\"SEED\":SEED})\n","\n","#                 # Log the fold number\n","#                 fold_run.config.update({\"fold\": fold + 1})\n","#                 print(f'Fold: {fold+1}')\n","\n","#                 wandb.log({\"info\": \"Loading Transformer Model \" + model_name})\n","#                 model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","                \n","\n","#                 # Train and validate your model, log metrics, etc.\n","#                 # ...\n","#                 # FINE-TUNING the model and obtaining the best model across all epochs\n","#                 fineTmodel = training(_wandb=fold_run, _model=model, _train_data=X_train, _val_data=X_val,\n","#                                     _learning_rate=config[\"learning\"], _optimizer_name=config[\"optimizer_name\"],\n","#                                     _schedule=config[\"schedule\"], _epochs=config[\"epochs\"], _tokenizer=tokenizer,\n","#                                     _batch_size=config[\"batch_size\"], _padding=\"max_length\", _max_length=config[\"max_length\"],\n","#                                     _truncation=True, _patience=config[\"patience\"], _measure=config[\"measure\"], _out=\"./out\")\n","\n","#                 # VALIDATING OR PREDICTING on the test partition, this time I'm using the validation set, but you have to use the test set.\n","#                 preds = validate(_wandb=fold_run, _model=fineTmodel, _test_data=X_val, _tokenizer=tokenizer,\n","#                                 _batch_size=config[\"batch_size\"], _padding=\"max_length\", _max_length=config[\"max_length\"],\n","#                                 _truncation=True, _measure=config[\"measure\"], evaltype=True)\n","                \n","#                 # Save the fine-tuned model to WandB\n","#                 model_artifact = wandb.Artifact(f'{lang}_{model_name.replace(\"/\", \"-\")}_run_{run_counter}_fold_{fold}_model', type='model')\n","#                 model_save_path = f'./models/{lang}_{model_name.replace(\"/\", \"-\")}_run_{run_counter}_fold_{fold}'\n","#                 os.makedirs(model_save_path, exist_ok=True)\n","#                 fineTmodel.save_pretrained(model_save_path)\n","#                 tokenizer.save_pretrained(model_save_path)\n","#                 model_artifact.add_dir(model_save_path)\n","#                 fold_run.log_artifact(model_artifact)\n","\n","#                 # Save predictions and corresponding test values to WandB\n","#                 preds_save_path = f\"./predictions/{lang}_{model_name.replace(\"/\", \"-\")}_run_{run_counter}_fold_{fold}.pkl\"\n","#                 os.makedirs(os.path.dirname(preds_save_path), exist_ok=True)\n","#                 with open(preds_save_path, \"wb\") as f:\n","#                     pickle.dump({\"predictions\": preds, \"test_values\": X_val}, f)\n","#                 preds_artifact = wandb.Artifact(f'{lang}_{model_name.replace(\"/\", \"-\")}_run_{run_counter}_fold_{fold}_predictions', type='predictions')\n","#                 preds_artifact.add_file(preds_save_path)\n","#                 fold_run.log_artifact(preds_artifact)\n","\n","#     # End the parent run\n","#     # parent_run.finish()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T14:30:51.295214Z","iopub.status.busy":"2024-05-27T14:30:51.294832Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading Tokenizer roberta-base\n","Iniciando proceso con los hiperparametros:\n","{'optimizer_name': 'rmsprop', 'learning': 5e-06, 'schedule': 'linear', 'patience': 5, 'epochs': 10, 'measure': 'mcc', 'batch_size': 64, 'max_length': 128}\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdavidandreuroqueta\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240527_143051-xplaemtv</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51/runs/xplaemtv' target=\"_blank\">english_roberta-base_1_fold_0</a></strong> to <a href='https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51' target=\"_blank\">https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51/runs/xplaemtv' target=\"_blank\">https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51/runs/xplaemtv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Fold: 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f4a2568f5ae4ad7a11f47d4f173de6a","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Usando 2 GPUs\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"20bb89314f7d4073b9d72b1420b2e4c5","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/3.82k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n","  with pd.option_context('mode.use_inf_as_na', True):\n","/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n","  with pd.option_context('mode.use_inf_as_na', True):\n"]},{"name":"stdout","output_type":"stream","text":["Usando 2 GPUs\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./models/english_roberta-base_run_1_fold_0)... Done. 1.9s\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='479.079 MB of 479.079 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_avg_loss</td><td>▁</td></tr><tr><td>test_mcc</td><td>▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▁▁▁</td></tr><tr><td>train_mcc</td><td>▁▅▅▆▇▇▇███</td></tr><tr><td>val_loss</td><td>▃▁▁▁▁▂▂▇█▄</td></tr><tr><td>val_mcc</td><td>▁▅▅▅▆██▄▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>divice</td><td>cuda</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>test_avg_loss</td><td>0.45699</td></tr><tr><td>test_mcc</td><td>0.74036</td></tr><tr><td>train_loss</td><td>0.05061</td></tr><tr><td>train_mcc</td><td>0.96182</td></tr><tr><td>val_loss</td><td>0.45699</td></tr><tr><td>val_mcc</td><td>0.74036</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">english_roberta-base_1_fold_0</strong> at: <a href='https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51/runs/xplaemtv' target=\"_blank\">https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51/runs/xplaemtv</a><br/> View project at: <a href='https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51' target=\"_blank\">https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51</a><br/>Synced 5 W&B file(s), 1 media file(s), 9 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240527_143051-xplaemtv/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240527_143706-ymi7botb</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51/runs/ymi7botb' target=\"_blank\">english_roberta-base_1_fold_1</a></strong> to <a href='https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51' target=\"_blank\">https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51/runs/ymi7botb' target=\"_blank\">https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51/runs/ymi7botb</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Fold: 2\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Usando 2 GPUs\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n","  with pd.option_context('mode.use_inf_as_na', True):\n","/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n","  with pd.option_context('mode.use_inf_as_na', True):\n"]},{"name":"stdout","output_type":"stream","text":["Usando 2 GPUs\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./models/english_roberta-base_run_1_fold_1)... Done. 2.2s\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='479.119 MB of 479.119 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_avg_loss</td><td>▁</td></tr><tr><td>test_mcc</td><td>▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▁▁▁</td></tr><tr><td>train_mcc</td><td>▁▅▆▆▇▇████</td></tr><tr><td>val_loss</td><td>█▃▁▁▃▅▄▅▇▅</td></tr><tr><td>val_mcc</td><td>▁▅▆▇▆▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>divice</td><td>cuda</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>test_avg_loss</td><td>0.35858</td></tr><tr><td>test_mcc</td><td>0.77338</td></tr><tr><td>train_loss</td><td>0.07914</td></tr><tr><td>train_mcc</td><td>0.93631</td></tr><tr><td>val_loss</td><td>0.35858</td></tr><tr><td>val_mcc</td><td>0.77338</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">english_roberta-base_1_fold_1</strong> at: <a href='https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51/runs/ymi7botb' target=\"_blank\">https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51/runs/ymi7botb</a><br/> View project at: <a href='https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51' target=\"_blank\">https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51</a><br/>Synced 5 W&B file(s), 1 media file(s), 9 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240527_143706-ymi7botb/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240527_144319-kjbatgbr</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51/runs/kjbatgbr' target=\"_blank\">english_roberta-base_1_fold_2</a></strong> to <a href='https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51' target=\"_blank\">https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51/runs/kjbatgbr' target=\"_blank\">https://wandb.ai/davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_14-30-51/runs/kjbatgbr</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Fold: 3\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Usando 2 GPUs\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]}],"source":["import os\n","import pickle\n","import numpy as np\n","import wandb\n","from datetime import datetime\n","from sklearn.model_selection import KFold\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.metrics import matthews_corrcoef\n","import torch\n","from sklearn.model_selection import train_test_split\n","\n","\n","# Get current date and time\n","current_datetime = datetime.now()\n","\n","# Format it to include hours, minutes, and seconds\n","formatted_datetime = current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n","\n","settings = wandb.Settings(start_method='thread', console='auto', mode='online')\n","\n","\n","wandb.login()\n","\n","SEED=1234\n","set_seed(SEED)\n","\n","preconfig = {\n","    0: {\n","        \"lang\": \"english\",\n","        \"model_name\": \"roberta-base\",\n","    },\n","#     1: {\n","#         \"lang\": \"english\",\n","#         \"model_name\": \"microsoft/deberta-base\",\n","#     },\n","    # 2: {\n","    #     \"lang\": \"spanish\",\n","    #     \"model_name\": \"dccuchile/bert-base-spanish-wwm-uncased\",\n","    # },\n","    # 3: {\n","    #     \"lang\": \"spanish\",\n","    #     \"model_name\": \"PlanTL-GOB-ES/roberta-base-bne\",\n","    # },\n","    # 4: {\n","    #     \"lang\": \"spanish\",\n","    #     \"model_name\": \"bert-base-multilingual-uncased\"\n","    # }\n","}\n","\n","hyperparams = {\n","    \"optimizer_name\": [\"rmsprop\", \"rmsprop\"], # [\"adam\", \"rmsprop\", \"sgd\"]\n","    \"learning\": [0.5e-5], # [0.5e-5, 1e-5, 0.5e-6, 1e-6\n","    \"schedule\": [\"linear\"], # [\"linear\", \"cosine\", \"constant\"]\n","    \"patience\": [5, 10], # [3, 5, 10]\n","    \"epochs\": [10, 20], # [5, 10, 20]\n","    \"measure\": [\"mcc\"],\n","    \"batch_size\": [64], # [16, 32, 64, 128]\n","    \"max_length\": [128]\n","}\n","\n","# Define KFold cross-validation\n","kf = KFold(n_splits=5)\n","\n","# For each preconfiguration\n","for i, preconfig in preconfig.items():\n","    lang = preconfig[\"lang\"]\n","    model_name = preconfig[\"model_name\"]\n","    \n","    if lang == \"spanish\":\n","        X= es_train_df\n","    elif lang == \"english\":\n","        X= en_train_df\n","        \n","    # Split 20% of the data into a separate dataset\n","    X_train, test_20_percent = train_test_split(X, test_size=0.20, random_state=SEED)\n","    \n","    # Create directory if it does not exist\n","    directory = f'data/test_20_percent_data_{lang}_{model_name}'\n","    os.makedirs(directory, exist_ok=True)\n","    \n","    # Save the 20% dataset to a CSV file\n","    csv_path = os.path.join(directory, 'data.csv')\n","    test_20_percent.to_csv(csv_path, index=False)\n","    \n","    X = X_train\n","    print(\"Loading Tokenizer \" + model_name)\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","    # Initialize a counter for the runs\n","    run_counter = 0\n","\n","    # For each hyperparameter configuration\n","    for config in product_dict(**hyperparams):\n","        run_counter += 1\n","        if config in hyperparams_covered:\n","            continue\n","        print(f\"Iniciando proceso con los hiperparametros:\\n{config}\")\n","            \n","        # For each fold\n","        for fold, (train_index, val_index) in enumerate(kf.split(X)):\n","            X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n","\n","            # Start a child run for this fold\n","            with wandb.init(project=f'lnr_oppositional_thinking_{formatted_datetime}',\n","                            entity='davidandreuroqueta',\n","                            group=f'{lang}_{model_name}',\n","                            job_type=f'hyperparam-tuning-{run_counter}',\n","                            name=f'{lang}_{model_name}_{run_counter}_fold_{fold}',\n","                            settings=settings) as fold_run:\n","                fold_run.config.update(preconfig)\n","                fold_run.config.update(config)\n","                fold_run.config.update({\"SEED\":SEED})\n","\n","                # Log the fold number\n","                fold_run.config.update({\"fold\": fold + 1})\n","                print(f'Fold: {fold+1}')\n","\n","                wandb.log({\"info\": \"Loading Transformer Model \" + model_name})\n","                model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","                \n","\n","                # Train and validate your model, log metrics, etc.\n","                # ...\n","                # FINE-TUNING the model and obtaining the best model across all epochs\n","                fineTmodel = training(_wandb=fold_run, _model=model, _train_data=X_train, _val_data=X_val,\n","                                    _learning_rate=config[\"learning\"], _optimizer_name=config[\"optimizer_name\"],\n","                                    _schedule=config[\"schedule\"], _epochs=config[\"epochs\"], _tokenizer=tokenizer,\n","                                    _batch_size=config[\"batch_size\"], _padding=\"max_length\", _max_length=config[\"max_length\"],\n","                                    _truncation=True, _patience=config[\"patience\"], _measure=config[\"measure\"], _out=\"./out\")\n","\n","                # VALIDATING OR PREDICTING on the test partition, this time I'm using the validation set, but you have to use the test set.\n","                preds = validate(_wandb=fold_run, _model=fineTmodel, _test_data=X_val, _tokenizer=tokenizer,\n","                                _batch_size=config[\"batch_size\"], _padding=\"max_length\", _max_length=config[\"max_length\"],\n","                                _truncation=True, _measure=config[\"measure\"], evaltype=True)\n","                \n","                # Save the fine-tuned model to WandB\n","                model_artifact = wandb.Artifact(f'{lang}_{model_name}_run_{run_counter}_fold_{fold}_model', type='model')\n","                model_save_path = f'./models/{lang}_{model_name}_run_{run_counter}_fold_{fold}'\n","                os.makedirs(model_save_path, exist_ok=True)\n","                fineTmodel.save_pretrained(model_save_path)\n","                tokenizer.save_pretrained(model_save_path)\n","                model_artifact.add_dir(model_save_path)\n","                fold_run.log_artifact(model_artifact)\n","\n","                # Save predictions and corresponding test values to WandB\n","                preds_save_path = f\"./predictions/{lang}_{model_name}_run_{run_counter}_fold_{fold}.pkl\"\n","                os.makedirs(os.path.dirname(preds_save_path), exist_ok=True)\n","                with open(preds_save_path, \"wb\") as f:\n","                    pickle.dump({\"predictions\": preds, \"test_values\": X_val}, f)\n","                preds_artifact = wandb.Artifact(f'{lang}_{model_name}_run_{run_counter}_fold_{fold}_predictions', type='predictions')\n","                preds_artifact.add_file(preds_save_path)\n","                fold_run.log_artifact(preds_artifact)\n","\n","    # End the parent run\n","    # parent_run.finish()\n"]},{"cell_type":"markdown","metadata":{},"source":["BAGGING"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:37:50.393361Z","iopub.status.busy":"2024-05-27T08:37:50.392669Z","iopub.status.idle":"2024-05-27T08:37:50.977275Z","shell.execute_reply":"2024-05-27T08:37:50.975849Z","shell.execute_reply.started":"2024-05-27T08:37:50.393328Z"},"trusted":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'data/extracted_20_percent_data_english_roberta-base/data.csv'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 18\u001b[0m\n\u001b[1;32m     11\u001b[0m test_data_paths \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish_roberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/extracted_20_percent_data_english_roberta-base/data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish_microsoft_deberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/extracted_20_percent_data_english_microsoft/deberta-base/data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Add paths for other configurations if needed\u001b[39;00m\n\u001b[1;32m     15\u001b[0m }\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Assuming y_test is included in the saved CSV files, load them\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m test_dfs \u001b[38;5;241m=\u001b[39m {key: pd\u001b[38;5;241m.\u001b[39mread_csv(path) \u001b[38;5;28;01mfor\u001b[39;00m key, path \u001b[38;5;129;01min\u001b[39;00m test_data_paths\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     19\u001b[0m y_test \u001b[38;5;241m=\u001b[39m test_dfs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish_roberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# Adjust according to your label column\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Initialize list to hold predictions for each model\u001b[39;00m\n","Cell \u001b[0;32mIn[20], line 18\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m test_data_paths \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish_roberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/extracted_20_percent_data_english_roberta-base/data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish_microsoft_deberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/extracted_20_percent_data_english_microsoft/deberta-base/data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Add paths for other configurations if needed\u001b[39;00m\n\u001b[1;32m     15\u001b[0m }\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Assuming y_test is included in the saved CSV files, load them\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m test_dfs \u001b[38;5;241m=\u001b[39m {key: \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, path \u001b[38;5;129;01min\u001b[39;00m test_data_paths\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     19\u001b[0m y_test \u001b[38;5;241m=\u001b[39m test_dfs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish_roberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# Adjust according to your label column\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Initialize list to hold predictions for each model\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/extracted_20_percent_data_english_roberta-base/data.csv'"]}],"source":["import os\n","import pickle\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from sklearn.metrics import matthews_corrcoef\n","\n","# Define SEED for reproducibility\n","SEED = 1234\n","\n","# Load the saved 20% test data\n","test_data_paths = {\n","    \"english_roberta-base\": \"data/extracted_20_percent_data_english_roberta-base/data.csv\",\n","#     \"english_microsoft_deberta-base\": \"data/extracted_20_percent_data_english_microsoft/deberta-base/data.csv\",\n","    # Add paths for other configurations if needed\n","}\n","\n","# Assuming y_test is included in the saved CSV files, load them\n","test_dfs = {key: pd.read_csv(path) for key, path in test_data_paths.items()}\n","y_test = test_dfs[\"label\"].values  # Adjust according to your label column\n","\n","# Initialize list to hold predictions for each model\n","all_fold_predictions = []\n","\n","# Define preconfig\n","preconfig = {\n","    0: {\n","        \"lang\": \"english\",\n","        \"model_name\": \"roberta-base\",\n","    },\n","#     1: {\n","#         \"lang\": \"english\",\n","#         \"model_name\": \"microsoft/deberta-base\",\n","#     }\n","# }\n","\n","# Perform voting for each preconfiguration\n","for i, config in preconfig.items():\n","    lang = config[\"lang\"]\n","    model_name = config[\"model_name\"]\n","    \n","    # Load tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(f'./models/{lang}_{model_name}_run_1_fold_0')\n","\n","    fold_predictions = []\n","\n","    for fold in range(5):\n","        # Load the saved model for each fold\n","        model_path = f'./models/{lang}_{model_name}_run_{fold+1}_fold_{fold}'\n","        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n","        model.eval()\n","\n","        # Tokenize the test data\n","        test_data = test_dfs[f\"{lang}_{model_name.replace('/', '_')}\"]\n","        test_encodings = tokenizer(test_data[\"feature1\"].tolist(), truncation=True, padding=True, max_length=128)\n","        test_dataset = Dataset.from_dict(test_encodings)\n","        test_loader = DataLoader(test_dataset, batch_size=64)\n","\n","        preds = []\n","        with torch.no_grad():\n","            for batch in test_loader:\n","                inputs = {key: val.to(model.device) for key, val in batch.items()}\n","                outputs = model(**inputs)\n","                preds.append(outputs.logits.cpu().numpy())\n","        \n","        preds = np.concatenate(preds, axis=0)\n","        fold_predictions.append(preds)\n","\n","    all_fold_predictions.append(fold_predictions)\n","\n","# Aggregate predictions by voting\n","aggregated_preds = np.mean(np.array(all_fold_predictions), axis=0)\n","final_preds = np.argmax(aggregated_preds, axis=1)\n","\n","# Evaluate the final predictions\n","mcc = matthews_corrcoef(y_test, final_preds)\n","\n","print(f'Bagging Model MCC: {mcc}')"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T11:54:07.746848Z","iopub.status.busy":"2024-05-27T11:54:07.746502Z","iopub.status.idle":"2024-05-27T11:54:15.311019Z","shell.execute_reply":"2024-05-27T11:54:15.309811Z","shell.execute_reply.started":"2024-05-27T11:54:07.746823Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact english_roberta-base_run_1_fold_0_model:latest, 478.72MB. 7 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n","Done. 0:0:3.7\n"]},{"ename":"KeyError","evalue":"'feature1'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n","File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'feature1'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Tokenize the test data\u001b[39;00m\n\u001b[1;32m     60\u001b[0m test_data \u001b[38;5;241m=\u001b[39m test_dfs[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 61\u001b[0m test_encodings \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist(), truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m     62\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict(test_encodings)\n\u001b[1;32m     63\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n","\u001b[0;31mKeyError\u001b[0m: 'feature1'"]}],"source":["import os\n","import pickle\n","import numpy as np\n","import wandb\n","import pandas as pd\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.metrics import matthews_corrcoef\n","\n","# Define SEED for reproducibility\n","SEED = 1234\n","\n","# Initialize WandB\n","wandb.login()\n","\n","# Load the saved 20% test data\n","test_data_paths = {\n","    \"english_roberta-base\": \"/kaggle/input/lnr-test-data/data.csv\",\n","}\n","\n","# Assuming y_test is included in the saved CSV files, load them\n","test_dfs = {key: pd.read_csv(path) for key, path in test_data_paths.items()}\n","y_test = test_dfs[\"english_roberta-base\"][\"label\"].values  # Adjust according to your label column\n","\n","# Initialize list to hold predictions for each model\n","all_fold_predictions = []\n","\n","# Define preconfig\n","preconfig = {\n","    0: {\n","        \"lang\": \"english\",\n","        \"model_name\": \"roberta-base\",\n","    },\n","}\n","\n","# Perform voting for each preconfiguration\n","for i, config in preconfig.items():\n","    lang = config[\"lang\"]\n","    model_name = config[\"model_name\"]\n","    \n","    # Initialize WandB run context\n","    run_name = f\"{lang}_{model_name}\"\n","    api = wandb.Api()\n","\n","    fold_predictions = []\n","\n","    for fold in range(5):\n","        # Load the model artifact from WandB\n","        artifact = api.artifact(f'davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_08-02-11/{run_name}_run_1_fold_{fold}_model:latest')\n","        artifact_dir = artifact.download()\n","        model_path = artifact_dir\n","\n","        # Load the model and tokenizer\n","        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n","        tokenizer = AutoTokenizer.from_pretrained(model_path)\n","        model.eval()\n","\n","        # Tokenize the test data\n","        test_data = test_dfs[f\"{lang}_{model_name.replace('/', '_')}\"]\n","        test_encodings = tokenizer(test_data[\"feature1\"].tolist(), truncation=True, padding=True, max_length=128)\n","        test_dataset = Dataset.from_dict(test_encodings)\n","        test_loader = DataLoader(test_dataset, batch_size=64)\n","\n","        preds = []\n","        with torch.no_grad():\n","            for batch in test_loader:\n","                inputs = {key: val.to(model.device) for key, val in batch.items()}\n","                outputs = model(**inputs)\n","                preds.append(outputs.logits.cpu().numpy())\n","        \n","        preds = np.concatenate(preds, axis=0)\n","        fold_predictions.append(preds)\n","\n","    all_fold_predictions.append(fold_predictions)\n","\n","# Aggregate predictions by voting\n","aggregated_preds = np.mean(np.array(all_fold_predictions), axis=0)\n","final_preds = np.argmax(aggregated_preds, axis=1)\n","\n","# Evaluate the final predictions\n","mcc = matthews_corrcoef(y_test, final_preds)\n","\n","print(f'Bagging Model MCC: {mcc}')"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T11:46:10.335263Z","iopub.status.busy":"2024-05-27T11:46:10.334947Z","iopub.status.idle":"2024-05-27T11:46:10.365229Z","shell.execute_reply":"2024-05-27T11:46:10.364301Z","shell.execute_reply.started":"2024-05-27T11:46:10.335238Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n","       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n","       0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n","       0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n","       1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n","       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n","       1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n","       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n","       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n","       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n","       1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n","       0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n","       0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n","       1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n","       0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n","       0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n","       0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n","       0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n","       0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n","       0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n","       1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n","       0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n","       1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n","       1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n","       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n","       1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n","       1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n","       0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n","       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n","       1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n","       1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n","       0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n","       1, 0, 1, 0, 0, 0, 0, 0])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["os.path.exists('/kaggle/input/lnr-test-data/data.csv')\n","\n","df = pd.read_csv('/kaggle/input/lnr-test-data/data.csv')\n","df['label'].values"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T12:04:40.125232Z","iopub.status.busy":"2024-05-27T12:04:40.124875Z","iopub.status.idle":"2024-05-27T12:13:14.303216Z","shell.execute_reply":"2024-05-27T12:13:14.301740Z","shell.execute_reply.started":"2024-05-27T12:04:40.125205Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-27 12:04:42.350529: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-27 12:04:42.350631: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-27 12:04:42.511160: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact english_roberta-base_run_1_fold_0_model:latest, 478.72MB. 7 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n","Done. 0:0:1.0\n"]},{"name":"stdout","output_type":"stream","text":["Columns in english_roberta-base: ['text', 'id', 'label']\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact english_roberta-base_run_1_fold_1_model:latest, 478.72MB. 7 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n","Done. 0:0:3.1\n"]},{"name":"stdout","output_type":"stream","text":["Columns in english_roberta-base: ['text', 'id', 'label']\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact english_roberta-base_run_1_fold_2_model:latest, 478.72MB. 7 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n","Done. 0:0:3.9\n"]},{"name":"stdout","output_type":"stream","text":["Columns in english_roberta-base: ['text', 'id', 'label']\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact english_roberta-base_run_1_fold_3_model:latest, 478.72MB. 7 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n","Done. 0:0:3.6\n"]},{"name":"stdout","output_type":"stream","text":["Columns in english_roberta-base: ['text', 'id', 'label']\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact english_roberta-base_run_1_fold_4_model:latest, 478.72MB. 7 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n","Done. 0:0:4.3\n"]},{"name":"stdout","output_type":"stream","text":["Columns in english_roberta-base: ['text', 'id', 'label']\n"]},{"ename":"ValueError","evalue":"Found input variables with inconsistent numbers of samples: [800, 5]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 93\u001b[0m\n\u001b[1;32m     90\u001b[0m final_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(aggregated_preds, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Evaluate the final predictions\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m mcc \u001b[38;5;241m=\u001b[39m \u001b[43mmatthews_corrcoef\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_preds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBagging Model MCC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmcc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:911\u001b[0m, in \u001b[0;36mmatthews_corrcoef\u001b[0;34m(y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatthews_corrcoef\u001b[39m(y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    849\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the Matthews correlation coefficient (MCC).\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \n\u001b[1;32m    851\u001b[0m \u001b[38;5;124;03m    The Matthews correlation coefficient is used in machine learning as a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m    -0.33...\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    912\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [800, 5]"]}],"source":["import os\n","import numpy as np\n","import wandb\n","import pandas as pd\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import matthews_corrcoef, accuracy_score\n","from datasets import Dataset as HFDataset\n","\n","# Define SEED for reproducibility\n","SEED = 1234\n","\n","# Initialize WandB\n","wandb.login()\n","\n","# Load the saved 20% test data\n","test_data_paths = {\n","    \"english_roberta-base\": \"/kaggle/input/lnr-test-data/data.csv\",\n","}\n","\n","# Assuming y_test is included in the saved CSV files, load them\n","test_dfs = {key: pd.read_csv(path) for key, path in test_data_paths.items()}\n","y_test = test_dfs[\"english_roberta-base\"][\"label\"].values  # Adjust according to your label column\n","\n","# Initialize list to hold predictions for each model\n","all_fold_predictions = []\n","\n","# Define preconfig\n","preconfig = {\n","    0: {\n","        \"lang\": \"english\",\n","        \"model_name\": \"roberta-base\",\n","    },\n","}\n","\n","# Perform voting for each preconfiguration\n","for i, config in preconfig.items():\n","    lang = config[\"lang\"]\n","    model_name = config[\"model_name\"]\n","    \n","    # Initialize WandB run context\n","    run_name = f\"{lang}_{model_name}\"\n","    api = wandb.Api()\n","\n","    fold_predictions = []\n","\n","    for fold in range(5):\n","        # Load the model artifact from WandB\n","        artifact = api.artifact(f'davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_08-02-11/{run_name}_run_1_fold_{fold}_model:latest')\n","        artifact_dir = artifact.download()\n","        model_path = artifact_dir\n","\n","        # Load the model and tokenizer\n","        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n","        tokenizer = AutoTokenizer.from_pretrained(model_path)\n","        model.eval()\n","\n","        # Tokenize the test data\n","        test_data = test_dfs[f\"{lang}_{model_name.replace('/', '_')}\"]\n","        \n","        # List columns to verify the correct column name\n","        print(f\"Columns in {lang}_{model_name.replace('/', '_')}: {test_data.columns.tolist()}\")\n","        \n","        # Adjust column name if necessary\n","        column_name = 'text'  # Change this to the correct column name based on the printed columns\n","        if column_name not in test_data.columns:\n","            raise ValueError(f\"Column '{column_name}' not found in test data columns: {test_data.columns.tolist()}\")\n","        \n","        test_encodings = tokenizer(test_data[column_name].tolist(), truncation=True, padding=True, max_length=128)\n","        test_dataset = HFDataset.from_dict(test_encodings)\n","        \n","        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","        test_loader = DataLoader(test_dataset, batch_size=64, collate_fn=data_collator)\n","\n","        preds = []\n","        with torch.no_grad():\n","            for batch in test_loader:\n","                inputs = {key: val.to(model.device) for key, val in batch.items()}\n","                outputs = model(**inputs)\n","                preds.append(outputs.logits.cpu().numpy())\n","        \n","        preds = np.concatenate(preds, axis=0)\n","        fold_predictions.append(preds)\n","\n","    all_fold_predictions.append(fold_predictions)\n","\n","# Aggregate predictions by voting\n","aggregated_preds = np.mean(np.array(all_fold_predictions), axis=0)\n","final_preds = np.argmax(aggregated_preds, axis=1)\n","\n","# Evaluate the final predictions\n","mcc = matthews_corrcoef(y_test, final_preds)\n","\n","print(f'Bagging Model MCC: {mcc}')\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T14:08:08.326764Z","iopub.status.busy":"2024-05-27T14:08:08.325812Z","iopub.status.idle":"2024-05-27T14:08:34.417328Z","shell.execute_reply":"2024-05-27T14:08:34.416316Z","shell.execute_reply.started":"2024-05-27T14:08:08.326726Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact english_roberta-base_run_1_fold_0_model:latest, 478.72MB. 7 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n","Done. 0:0:1.0\n"]},{"name":"stdout","output_type":"stream","text":["Columns in english_roberta-base: ['text', 'id', 'label']\n","Usando 2 GPUs\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact english_roberta-base_run_1_fold_1_model:latest, 478.72MB. 7 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n","Done. 0:0:0.9\n"]},{"name":"stdout","output_type":"stream","text":["Columns in english_roberta-base: ['text', 'id', 'label']\n","Usando 2 GPUs\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact english_roberta-base_run_1_fold_2_model:latest, 478.72MB. 7 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n","Done. 0:0:1.0\n"]},{"name":"stdout","output_type":"stream","text":["Columns in english_roberta-base: ['text', 'id', 'label']\n","Usando 2 GPUs\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact english_roberta-base_run_1_fold_3_model:latest, 478.72MB. 7 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n","Done. 0:0:0.9\n"]},{"name":"stdout","output_type":"stream","text":["Columns in english_roberta-base: ['text', 'id', 'label']\n","Usando 2 GPUs\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact english_roberta-base_run_1_fold_4_model:latest, 478.72MB. 7 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n","Done. 0:0:0.9\n"]},{"name":"stdout","output_type":"stream","text":["Columns in english_roberta-base: ['text', 'id', 'label']\n","Usando 2 GPUs\n","Bagging Model MCC: 0.7514312061446426\n"]}],"source":["import os\n","import numpy as np\n","import wandb\n","import pandas as pd\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from sklearn.metrics import matthews_corrcoef\n","from collections import Counter\n","\n","# Define SEED for reproducibility\n","SEED = 1234\n","\n","# Initialize WandB\n","wandb.login()\n","\n","# Define the validation function\n","def validate(_wandb, _model, _test_data, _tokenizer, _batch_size=32, _padding=\"max_length\", _max_length=512, _truncation=True, _measure=\"accuracy\", evaltype=True):\n","    # Tokenización de los datos de prueba\n","    test_encodings = _tokenizer(_test_data['text'].tolist(), max_length=_max_length, truncation=_truncation, padding=_padding, return_tensors=\"pt\")\n","    test = MyDataset(test_encodings, _test_data[\"label\"].tolist())\n","    test_dataloader = torch.utils.data.DataLoader(test, batch_size=_batch_size)\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","    _model.to(device)  # Mueve el modelo al dispositivo adecuado\n","    if torch.cuda.device_count() > 1:\n","        print(f\"Usando {torch.cuda.device_count()} GPUs\")\n","        _model = torch.nn.DataParallel(_model)\n","\n","    if evaltype:\n","        eval_metric = evaluate.load(f\"Yeshwant123/{_measure}\")\n","\n","    _model.eval()\n","    total_loss = 0\n","    all_predictions = []\n","\n","    with torch.no_grad():\n","        for batch in test_dataloader:\n","            batch = {k: v.to(device) for k, v in batch.items()}\n","            outputs = _model(**batch)\n","            loss = outputs.loss\n","            if loss.dim() > 0:\n","                loss = loss.mean()  # Asegura que la pérdida es un escalar\n","            logits = outputs.logits\n","            predictions = torch.argmax(logits, dim=-1)\n","            all_predictions.append(predictions.cpu())  # Move predictions to CPU\n","            total_loss += loss.item()\n","            if evaltype:\n","                eval_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n","\n","    # Concatenar todas las predicciones\n","    all_predictions = torch.cat(all_predictions).cpu().numpy()  # Ensure tensor is on CPU before converting to numpy\n","\n","    if evaltype:\n","        total_acc_test = eval_metric.compute()\n","        test_mesure = total_acc_test[_measure]\n","        avg_test_loss = total_loss / len(test_dataloader)\n","        _wandb.log({\n","            f'test_{_measure}': test_mesure,\n","            'test_avg_loss': avg_test_loss\n","        })\n","\n","    return all_predictions\n","\n","# Load the saved 20% test data\n","test_data_paths = {\n","    \"english_roberta-base\": \"/kaggle/input/lnr-test-data/data.csv\",\n","}\n","\n","# Assuming y_test is included in the saved CSV files, load them\n","test_dfs = {key: pd.read_csv(path) for key, path in test_data_paths.items()}\n","y_test = test_dfs[\"english_roberta-base\"][\"label\"].values  # Adjust according to your label column\n","\n","# Initialize matrix to hold predictions for each fold (800 samples x 5 folds)\n","num_folds = 5\n","num_samples = len(y_test)\n","fold_predictions = np.zeros((num_samples, num_folds))\n","\n","# Define preconfig\n","preconfig = {\n","    0: {\n","        \"lang\": \"english\",\n","        \"model_name\": \"roberta-base\",\n","    },\n","}\n","\n","# Perform voting for each preconfiguration\n","for i, config in preconfig.items():\n","    lang = config[\"lang\"]\n","    model_name = config[\"model_name\"]\n","    \n","    # Initialize WandB run context\n","    run_name = f\"{lang}_{model_name}\"\n","    api = wandb.Api()\n","\n","    for fold in range(num_folds):\n","        # Load the model artifact from WandB\n","        artifact = api.artifact(f'davidandreuroqueta/lnr_oppositional_thinking_2024-05-27_08-02-11/{run_name}_run_1_fold_{fold}_model:latest')\n","        artifact_dir = artifact.download()\n","        model_path = artifact_dir\n","\n","        # Load the model and tokenizer\n","        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n","        tokenizer = AutoTokenizer.from_pretrained(model_path)\n","        model.eval()\n","\n","        # Tokenize the test data\n","        test_data = test_dfs[f\"{lang}_{model_name.replace('/', '_')}\"]\n","        \n","        # List columns to verify the correct column name\n","        print(f\"Columns in {lang}_{model_name.replace('/', '_')}: {test_data.columns.tolist()}\")\n","        \n","        # Adjust column name if necessary\n","        column_name = 'text'  # Change this to the correct column name based on the printed columns\n","        if column_name not in test_data.columns:\n","            raise ValueError(f\"Column '{column_name}' not found in test data columns: {test_data.columns.tolist()}\")\n","        \n","        predictions = validate(_wandb=wandb, _model=model, _test_data=test_data, _tokenizer=tokenizer, _batch_size=64, _padding=\"max_length\", _max_length=128, _truncation=True, _measure=\"accuracy\", evaltype=False)\n","        \n","        fold_predictions[:, fold] = predictions\n","\n","# Compute the majority vote for each sample\n","final_preds = [Counter(fold_predictions[i, :]).most_common(1)[0][0] for i in range(num_samples)]\n","\n","# Evaluate the final predictions\n","mcc = matthews_corrcoef(y_test, final_preds)\n","\n","print(f'Bagging Model MCC: {mcc}')"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T14:15:53.753225Z","iopub.status.busy":"2024-05-27T14:15:53.752552Z","iopub.status.idle":"2024-05-27T14:15:53.760346Z","shell.execute_reply":"2024-05-27T14:15:53.759347Z","shell.execute_reply.started":"2024-05-27T14:15:53.753192Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Bagging Model Accuracy: 0.8875\n"]}],"source":["from sklearn.metrics import matthews_corrcoef, accuracy_score\n","\n","accuracy = accuracy_score(y_test, final_preds)\n","\n","print(f'Bagging Model Accuracy: {accuracy}')\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5044153,"sourceId":8461645,"sourceType":"datasetVersion"},{"datasetId":5094523,"sourceId":8530315,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"13e22342799a4323bf3f1059e082827f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15bad4d6a1164c70acbc8db7edbd9e66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_62607de3071e4999951dd21f2d7422c4","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_929d8dcca0024332b37e7e5c4b6ef00e","value":1}},"21f30a65139d4aa6b755660371049a03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30e03ce93eb34293bcd87ca7959854a3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3407d57d8488404eaff1306a93b3b4ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_b822b37bb5cd40e58f423e83a82a61e2","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93271ff93bec44f583c1a4622529f1ce","value":1}},"3946ec0b82444435bbef194b589ce92d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_7551706e14434d37885ece328004c5ec","IPY_MODEL_e8cff45d1e774b3bab4dd6befb10d43c"],"layout":"IPY_MODEL_e67937bd65224880a199beffec88be89"}},"3f0b92538fc04fd08cd2e1572de89f84":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42d5bd90622e4626b8f73ab130cf846f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_a027348f0aaf4a3bb4e354e9b73bd8ef","IPY_MODEL_ffdd2c63a036471ebf0692a844fc12d8"],"layout":"IPY_MODEL_f3c6d847ab604a57aa04ab3f2ba258e2"}},"62607de3071e4999951dd21f2d7422c4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7092d9f13ee9493f95362b26e23f601e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7551706e14434d37885ece328004c5ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c987fd2de8b405a8d294975214cbc3d","placeholder":"​","style":"IPY_MODEL_7092d9f13ee9493f95362b26e23f601e","value":"0.013 MB of 0.013 MB uploaded\r"}},"7921f8a3b21645d9afded2d1ef2b59ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a5d4ee7078b4f67965c4ccb6b793625":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c987fd2de8b405a8d294975214cbc3d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cedcf6632e643b383e8d9c196af693a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"929d8dcca0024332b37e7e5c4b6ef00e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"93271ff93bec44f583c1a4622529f1ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9ce3a42132254f7ea112342a71a658c6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a027348f0aaf4a3bb4e354e9b73bd8ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30e03ce93eb34293bcd87ca7959854a3","placeholder":"​","style":"IPY_MODEL_7921f8a3b21645d9afded2d1ef2b59ba","value":"0.014 MB of 0.014 MB uploaded\r"}},"aa8399df41354082b2923f1e99075ed9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f0b92538fc04fd08cd2e1572de89f84","placeholder":"​","style":"IPY_MODEL_b71b0e1ce144434c9fc9b4d9f781997c","value":"0.014 MB of 0.014 MB uploaded\r"}},"b48d92ae81684a96be46844484e156fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_e7a687efc09840a280c42bf8f3d5e4a5","IPY_MODEL_15bad4d6a1164c70acbc8db7edbd9e66"],"layout":"IPY_MODEL_21f30a65139d4aa6b755660371049a03"}},"b71b0e1ce144434c9fc9b4d9f781997c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b822b37bb5cd40e58f423e83a82a61e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccd6079762884df5b2ec6a5ffb5a47bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e18ed07e2eea4fce8dd7a06d5edc5317":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e67937bd65224880a199beffec88be89":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7a687efc09840a280c42bf8f3d5e4a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7cedcf6632e643b383e8d9c196af693a","placeholder":"​","style":"IPY_MODEL_ccd6079762884df5b2ec6a5ffb5a47bc","value":"0.014 MB of 0.014 MB uploaded\r"}},"e8cff45d1e774b3bab4dd6befb10d43c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_13e22342799a4323bf3f1059e082827f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e18ed07e2eea4fce8dd7a06d5edc5317","value":1}},"eb5dc2642e2345e0a5582a4f94451389":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ef4721235ee240d08badd81528eaf79c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_aa8399df41354082b2923f1e99075ed9","IPY_MODEL_3407d57d8488404eaff1306a93b3b4ac"],"layout":"IPY_MODEL_7a5d4ee7078b4f67965c4ccb6b793625"}},"f3c6d847ab604a57aa04ab3f2ba258e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffdd2c63a036471ebf0692a844fc12d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ce3a42132254f7ea112342a71a658c6","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eb5dc2642e2345e0a5582a4f94451389","value":1}}}}},"nbformat":4,"nbformat_minor":4}
